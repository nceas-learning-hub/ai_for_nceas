[
  {
    "objectID": "s03_ai_lit_review.html",
    "href": "s03_ai_lit_review.html",
    "title": "Collaborating with Git and GitHub",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nApply the principles, features, and collaboration tools of Git and GitHub to effectively collaborate with colleagues on code\nAnalyze and evaluate common causes of conflicts that arise when collaborating on repositories\nDemonstrate the ability to resolve conflicts using Git conflict resolution techniques\nApply workflows and best practices that minimize conflicts on collaborative repositories",
    "crumbs": [
      "Collaborating with Git and GitHub"
    ]
  },
  {
    "objectID": "s01_ai_coding_support.html",
    "href": "s01_ai_coding_support.html",
    "title": "AI for Coding Support",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nActive coding support in RStudio with GitHub Copilot:\n\nConfiguring RStudio to work with GitHub Copilot\nExploring techniques to use Copilot to assist with coding tasks\n\nIterative coding support with ChatGPT:\n\nTranslating pseudocode into functional code\nCommenting existing code\nSuggesting improvements to code for performance or readability\nWriting unit tests for functions\nTranslating code from one language to another (e.g., R vs. Python)",
    "crumbs": [
      "AI for Coding Support"
    ]
  },
  {
    "objectID": "s01_ai_coding_support.html#active-coding-support-with-github-copilot",
    "href": "s01_ai_coding_support.html#active-coding-support-with-github-copilot",
    "title": "AI for Coding Support",
    "section": "1 Active coding support with GitHub Copilot",
    "text": "1 Active coding support with GitHub Copilot\nGitHub Copilot is an AI-powered code completion tool that helps you write code faster and with fewer errors. It is available as a plugin for IDEs such as Visual Studio Code and RStudio. Copilot is a large language model (LLM) related to ChatGPT, trained on GitHub and other codebases, and it can generate code completions in real time based on the context of your coding task. In this lesson, we will configure RStudio to work with GitHub Copilot and practice techniques to use Copilot to assist with coding tasks.\nIf you have an education GitHub account (e.g., an .edu email address that is associated with your GitHub account), you should be able to get free access to Copilot. Otherwise you may need to pay for a subscription. See more Copilot requirements here.\n\n\n\n\n\n\nNote\n\n\n\nHere we will focus on using AI code completion tools in R and RStudio, since it is a popular language and IDE for ecologists, but the same methods work for other languages and IDEs. VS Code is a Microsoft product, as is Copilot (and GitHub for that matter), so its integration is even better than RStudio.\n\n\n\n1.1 Configure RStudio to work with GitHub Copilot\nIn RStudio, open the “Tools” menu and scroll down to the “Global Options” menu item. In the dialog box, at the bottom of the left hand sidebar, you should see “Copilot” as an option (if not, perhaps you have an old version of RStudio, requires 2023.09.0 or later!).\n\n\n\n\n\nClick on the checkbox for “Enable GitHub Copilot.” You may wish to change some of the other options: setting Copilot to auto-suggest completions or only show completions when asked using ctrl-\\, or changing the delay time before Copilot starts giving suggestions.\nNext, click the “sign in” button under the “Enable GitHub Copilot” checkbox. This will open dialog box with a link and an eight-digit verification code:\n\n\n\n\n\nClick on the link, enter the verification code, and sign in with your GitHub account.\n\n\n\n\n\nOnce you’ve entered the authorization code, you will need to authorize RStudio to use Copilot:\n\n\n\n\n\nNow you should be good to go! Open up a new Quarto document and start typing, see if it starts autocompleting your sentences or code.\n\n\n1.2 Using Copilot to assist with coding tasks\nYou need a fundamental understanding of coding to be able to use Copilot effectively. It can make intelligent guesses as to your intentions, but doesn’t necessarily know where your files are, or what you intend to do with the data. So it is helpful to guide Copilot in the right direction using comments, sentences, and pseudocode before you start any actual coding. The more information you provide through your Markdown text and code-chunk comments, the better Copilot will be able to help you.\n\n\n\n\n\n\nExercise 1\n\n\n\n\n\n\nCreate a code chunk, read in some data, do some data wrangling, and then make a plot with the resulting data. Use comments in your code chunk to signal to Copilot what you’d like to do at each step. Here are the steps we will do, using a tidyverse workflow:\n\nload the palmerpenguins package and load the penguins data\ndrop any incomplete observations (any NA values)\nfilter to just keep the Chinstrap and Adelie species\ngenerate a scatter plot of bill length vs. bill depth for each species\nthen, for the two species, run a t-test to determine the degree to which the bill lengths are statistically different between the two species. Print a summary of the t test results.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nHere’s what it looked like when I let Copilot help me write the code - I accepted each line by pressing “tab”. This might change entirely if I ran the exercise again. How different does it look from the code you wrote?\n\n\nCode\n### Load the tidyverse and palmer penguins packages\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(palmerpenguins)\n\n### Load the penguins data\ndata(\"penguins\")\n\n### Filter to just keep the Chinstrap and Adelie species\npenguins_filtered &lt;- penguins %&gt;%\n  drop_na() %&gt;%\n  filter(species %in% c(\"Chinstrap\", \"Adelie\"))\n\n### Generate a scatter plot of bill length vs. bill depth for each species\nggplot(penguins_filtered, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point() +\n  labs(title = \"Bill length vs. bill depth for Chinstrap and Adelie penguins\",\n       x = \"Bill length (mm)\",\n       y = \"Bill depth (mm)\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nCode\n### Run a t-test to determine the degree to which the bill lengths are statistically different between the two species\nt.test(bill_length_mm ~ species, data = penguins_filtered)\n\n\n\n    Welch Two Sample t-test\n\ndata:  bill_length_mm by species\nt = -21.712, df = 108.17, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Adelie and group Chinstrap is not equal to 0\n95 percent confidence interval:\n -10.92365  -9.09605\nsample estimates:\n   mean in group Adelie mean in group Chinstrap \n               38.82397                48.83382 \n\n\nCode\n### Print a summary of the t test results as a dataframe\nt.test(bill_length_mm ~ species, data = penguins_filtered) %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nestimate\nestimate1\nestimate2\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n-10.00985\n38.82397\n48.83382\n-21.7125\n0\n108.1722\n-10.92365\n-9.09605\nWelch Two Sample t-test\ntwo.sided\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPseudocode\n\n\n\nPseudocode is a way of planning out an analysis by writing out the sequence of steps you expect to take, in plain language, perhaps as bullet points, just as we did above. Some tips:\n\nSpecify if you want to use the tidyverse metapackage or base R functions.\nIf you know the names of certain functions, it is helpful to use those in your pseudocode, to help Copilot incorporate those into its code completions.\nUse comments to signal to Copilot what you’d like to do at each step.\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\n\n\nLet’s try the same exercise but this time let’s only use base R functions:\n\nload the palmerpenguins package and load the penguins data\ndrop incomplete observations\nfilter to just keep the Chinstrap and Adelie species\ngenerate a scatter plot of bill length vs. bill depth for each species\nthen, for the two species, run a t-test to determine the degree to which the bill lengths are statistically different between the two species. Print a summary of the t test results.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nHere is what Copilot came up with for me. Note, it knew from above that I wanted to use base R functions this time instead of tidyverse functions!\n\n### Load the palmer penguins package and load the penguins data\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\n### Drop incomplete observations\npenguins &lt;- na.omit(penguins)\n\n### Filter to just keep the Chinstrap and Adelie species\npenguins_filtered &lt;- penguins[penguins$species %in% c(\"Chinstrap\", \"Adelie\"), ]\n\n### Generate a scatter plot of bill length vs. bill depth for each species\nplot(penguins_filtered$bill_length_mm, penguins_filtered$bill_depth_mm, col = penguins_filtered$species)\n\n\n\n\n\n\n\n### Print a summary of the t test results\nsummary(t.test(penguins_filtered$bill_length_mm ~ penguins_filtered$species))\n\n            Length Class  Mode     \nstatistic   1      -none- numeric  \nparameter   1      -none- numeric  \np.value     1      -none- numeric  \nconf.int    2      -none- numeric  \nestimate    2      -none- numeric  \nnull.value  1      -none- numeric  \nstderr      1      -none- numeric  \nalternative 1      -none- character\nmethod      1      -none- character\ndata.name   1      -none- character\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCopilot limitations!\n\n\n\nCopilot cannot read your mind, and it may make incorrect assumptions about what you want to do with your data and analysis. It is important to understand the code that Copilot is generating for you, and to check it for errors! You get the last word (no matter how hard Copilot tries!), and you as the coder are responsible for the final product.\nKeep in mind that you can always reject Copilot’s suggestions and write your own code, or accept its suggestions as a starting point then edit them as you see fit.\n\n\n\n\n1.3 Using Copilot for more complex coding tasks\nCopilot can also help you write functions and loops, among other things.\n\n1.3.1 Copilot to write a function\n\n\n\n\n\n\nExercise 3\n\n\n\n\n\n\nLet’s write a function that takes a dataframe of data and for two given columns, calculates the slope, intercept, and p value for a linear regression model. Let’s call it calc_lm(). Some pseudocode to get you started:\n\nidentify and pull out the y (outcome) and x (predictor) variables as columns in the data frame\nperform a linear regression on y ~ x\nreturn the slope, intercept, and p value in a dataframe.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nHere’s what Copilot suggested for me, line by line, as I was writing this lesson. After finishing the function, I started writing a comment to test, it assumed I wanted to use data that I had mentioned earlier in the script, i.e., penguins_filtered.\n\n# function to calculate slope, intercept, and p value for a linear regression model\ncalc_lm &lt;- function(data, y, x) {\n  # identify and pull out the y (outcome) and x (predictor) variables as columns in the data frame\n  y &lt;- data[[y]]\n  x &lt;- data[[x]]\n  \n  # perform a linear regression on y ~ x\n  lm_model &lt;- lm(y ~ x, data = data)\n  \n  # return the slope, intercept, and p value in a dataframe\n  data.frame(slope = coef(lm_model)[2],\n             intercept = coef(lm_model)[1],\n             p_value = summary(lm_model)$coefficients[2, 4])\n}\n### test on the penguins data\ncalc_lm(penguins_filtered, \"bill_length_mm\", \"bill_depth_mm\")\n\n     slope intercept      p_value\nx 1.263017  18.80234 4.863481e-05\n\n\nDoes Copilot’s suggestion work for you? If not take a moment to fix it and make it work. For me, it created a perfectly good function the first time. Some observations on the code presented here:\n\nI would have avoided overwriting the local value of x and y in the function, as it could be confusing.\nNote that the p value it selects from the linear model is that of the slope, not the intercept. That is probably what we wanted, but would be a good idea to make sure that meets our expectations!\nI might also prefer to have the output in a different format, perhaps using broom::tidy().\n\n\n\n\n\n\n\n\n\n\n\n1.3.2 Copilot to write a loop\n\n\n\n\n\n\nExercise 4\n\n\n\n\n\n\nNow let’s get Copilot to help use the previous function to iterate over the various species by sex. Using for-loops, this will involve nested loops (one for species, one for sex), so let’s see if Copilot can figure that out! Pseudocode:\n\nFor each species, and each sex, apply the function above to the penguins data, and add the species and sex to the resulting dataframe.\nStore the results in a list of dataframes\nThen combine the results into a single dataframe.\n\n\n\n\n\n\n\nWarning\n\n\n\nI almost certainly wouldn’t have actually used a for loop here in the first place - better options would be a group_by() %&gt;% summarize() workflow or purrr::map2 - but we’re just trying to see what Copilot can do with a more complex coding task!\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nHere’s what Copilot suggested, based on the pseudocode and previous code chunks. However, this was a little more complex and required some guidance, hand holding, and bug-fixing.\n\n# create an empty list to store the results\n1results &lt;- list()\n2i &lt;- 1\n\n# iterate over the species\nfor (species in unique(penguins_filtered$species)) {\n  # iterate over the sex\n3  for(sex in unique(penguins_filtered$sex)) {\n    # apply the function above to the penguins data\n4    result &lt;- calc_lm(penguins_filtered[penguins_filtered$species == species & penguins_filtered$sex == sex, ], \"bill_length_mm\", \"bill_depth_mm\")\n    # add the species and sex to the resulting dataframe\n    result$species &lt;- species\n5    result$sex &lt;- sex\n    # store the results in the list\n6    results[[i]] &lt;- result\n7    i &lt;- i + 1\n  }\n}\n\n# combine the results into a single dataframe\nresults_df &lt;- do.call(rbind, results)\n\n\n1\n\nCopilot didn’t pre-allocate space for the list based on the number of expected entries. Not a dealbreaker for a small project, but generally a good idea.\n\n2\n\nAfter Copilot started the loop, it didn’t include an indexing variable, and hadn’t considered that the “species” index would be character, not numeric, so would be challenging to use as an indexing variable after the fact. I manually added in an index i to keep track of the list index.\n\n3\n\nCopilot struggled with this nested loop format - so I had to manually start typing this line; but then Copilot was able to finish.\n\n4\n\nAgain, Copilot struggled here - it wrote the function call halfway through the filtered step; when I wrote the penguins_filtered$sex == sex part, it was able to finish the line. However, it made some errors finishing the line as well!\n\n5\n\nOnce again, Copilot wasn’t sure how to add the sex info to the resulting dataframe, so I had to manually start writing.\n\n6\n\nHere, Copilot used a single bracket for indexing, which is incorrect for a list in R. I had to manually change this to double brackets.\n\n7\n\nCopilot didn’t include an incrementing index variable, so I had to manually add this.\n\n\n\n\nIn summary, Copilot was helpful in getting the ball rolling here, but needed lots of manual help. Additionally, it made some choices I probably would have made differently - e.g., I probably would have filtered the penguins_filtered dataframe before the calc_lm call, and probably would have used dplyr::mutate to add the species and sex columns to the result. Finally, I probably would have used bind_rows() instead of do.call(rbind, results) to combine the results, but that’s just what I’m used to.\n\n\n\n\n\n\n\n\n\n\n\n1.4 Copilot for coding: summary\nCopilot can be very helpful at speeding up your coding, especially for simple or well-defined tasks. One potential pitfall is that using Copilot in this way has a certain amount of path dependency. Once a bit of code is written in the script, Copilot takes that as given - it can’t modify or erase code, it can only keep going forward. So it is important to monitor your progress and make sure Copilot is not leading you astray. For example, if you prefer tidyverse workflow but Copilot is suggesting base R functions, you may need to intervene and guide Copilot in the right direction.\nIn the next section, we’ll look at how to use ChatGPT to help with coding in a more interactive/iterative fashion, which can avoid path dependency but may introduce other complexities.\n\n\n\n\n\n\nSummary: Copilot tips\n\n\n\n\nBefore starting your analysis, prime Copilot by writing out an overview of your analysis, including packages, functions, and statistical tests you want to do.\nWhile writing code, if Copilot gets stuck, or treads down an unproductive path, you have several options:\n\nwrite out a comment or additional pseudocode to guide Copilot in the right direction.\naccept Copilot’s suggestion (if it’s on the right track) and manually fix it before continuing.\nreject Copilot’s suggestion (if it’s way off base) and write your own code, to signal what you really want\n\nRun the code to make sure it works as expected, and check for errors. You are responsible for the final result.\nRemember, you can turn Copilot off entirely, or set it to autocomplete only when requested:\n\nTools –&gt; Global Options –&gt; Copilot –&gt; Show Copilot code suggestions: Manually (Ctrl-Backslash)).",
    "crumbs": [
      "AI for Coding Support"
    ]
  },
  {
    "objectID": "s01_ai_coding_support.html#iterative-coding-support-with-chatgpt",
    "href": "s01_ai_coding_support.html#iterative-coding-support-with-chatgpt",
    "title": "AI for Coding Support",
    "section": "2 Iterative coding support with ChatGPT",
    "text": "2 Iterative coding support with ChatGPT\nIn this section we’ll look at using ChatGPT outside of RStudio. Here we can take existing code and feed it into ChatGPT and ask it to help us with various tasks, or feed pseudocode into ChatGPT and ask it to generate code for us.\n\n2.1 Pseudocode into code\nJust as Copilot can generate code from pseudocode, ChatGPT can do the same. You can then copy/paste the results into RStudio.\n\n\n\n\n\n\nExercise 5\n\n\n\n\n\n\nLet’s give ChatGPT the same sequence of steps we used above. We will copy and paste the pseudocode from above (repeated here) into ChatGPT and ask it to generate code for us.\n\nload the palmerpenguins package and load the penguins data\ndrop any incomplete observations (any NA values)\nfilter to just keep the Chinstrap and Adelie species\ngenerate a scatter plot of bill length vs. bill depth for each species\nthen, for the two species, run a t-test to determine the degree to which the bill lengths are statistically different between the two species. Print a summary of the t test results.\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nHere is the code it generated for me:\n\n# Load necessary packages\nlibrary(palmerpenguins)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Load the penguins dataset and remove incomplete observations\npenguins_clean &lt;- penguins %&gt;%\n  na.omit() %&gt;%\n  filter(species %in% c(\"Chinstrap\", \"Adelie\"))\n\n# Generate scatter plot of bill length vs. bill depth\nggplot(penguins_clean, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point() +\n  labs(title = \"Bill Length vs. Bill Depth\",\n       x = \"Bill Length (mm)\",\n       y = \"Bill Depth (mm)\") +\n  theme_minimal()\n\n# Perform a t-test on bill length between Chinstrap and Adelie species\nt_test_result &lt;- t.test(bill_length_mm ~ species, data = penguins_clean)\n\n# Print summary of t-test results\nprint(t_test_result)\n\nThis code works exactly as desired, though we set it to not evaluate here in the script, for sake of space.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 6\n\n\n\n\n\n\nI could follow up with a request to not use any tidyverse packages…\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nAnd this is what it came up with:\n\n# Load the palmerpenguins package\nlibrary(palmerpenguins)\n\n# Load the dataset and remove incomplete observations\npenguins_clean &lt;- na.omit(penguins)\n\n# Keep only Chinstrap and Adelie species\npenguins_clean &lt;- penguins_clean[penguins_clean$species %in% c(\"Chinstrap\", \"Adelie\"), ]\n\n# Generate scatter plot of flipper length vs. bill depth\nplot(penguins_clean$bill_length_mm, penguins_clean$bill_depth_mm,\n     col = ifelse(penguins_clean$species == \"Chinstrap\", \"blue\", \"red\"),\n     pch = 19,\n     xlab = \"Bill Length (mm)\",\n     ylab = \"Bill Depth (mm)\",\n     main = \"Bill Length vs. Bill Depth\")\nlegend(\"topright\", legend = c(\"Chinstrap\", \"Adelie\"), col = c(\"blue\", \"red\"), pch = 19)\n\n# Perform a t-test on flipper length between Chinstrap and Adelie species\nt_test_result &lt;- t.test(bill_length_mm ~ species, data = penguins_clean)\n\n# Print summary of t-test results\nprint(t_test_result)\n\nThis code works exactly as desired, though we set it to not evaluate here in the script, for sake of space.\n\n\n\n\n\n\n\n\n\n\n2.2 Commenting existing code\nOften when we are trying to code, we Google to find a solution online (e.g., Stack Overflow), but the code is not familiar to us or difficult to understand. ChatGPT can automatically take that code and insert helpful comments to explain what each line is doing.\n\n\n\n\n\n\nExercise 7\n\n\n\n\n\n\nHere is a complex Python script that would be challenging to understand for someone not proficient in Python:\nhttps://raw.githubusercontent.com/convei-wwf/NLParty/refs/heads/main/abstract_classifier.py\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nHere’s what it came up with! (This is a long script, so code folding is enabled)\n\n\nCode\nimport argparse  # For parsing command-line arguments\nimport glob  # For file pattern matching\nimport logging  # For logging messages\nimport os  # For operating system interactions\nimport time  # For measuring execution time\nimport re  # For regular expressions\nimport warnings  # For handling warnings\nfrom transformers import pipeline  # For using Hugging Face's transformers pipeline\nimport torch  # For PyTorch operations\n\n# Suppress specific UserWarnings from torch.utils.data\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.utils.data\")\n\n# Configure logging settings\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format=(\n        '%(asctime)s (%(relativeCreated)d) %(levelname)s %(name)s'\n        ' [%(funcName)s:%(lineno)d] %(message)s'\n    )\n)\nlogging.getLogger('taskgraph').setLevel(logging.INFO)\nLOGGER = logging.getLogger(__name__)\n\ndef main():\n    # Check if CUDA (GPU support) is available and print the result\n    print(torch.cuda.is_available())\n\n    # Set up argument parser for command-line inputs\n    parser = argparse.ArgumentParser(description='Affiliation classifier')\n    parser.add_argument('bib_file_pattern', help='path to bibliography list')\n    parser.add_argument('abstract_tag_file', help='path to abstract tags')\n    parser.add_argument('--target_path', help='target classified table')\n    args = parser.parse_args()\n\n    # Load bibliography files matching the given pattern\n    print('load bib_file')\n    affiliation_set = set()\n    article_count = 0\n    for bib_file in glob.glob(args.bib_file_pattern):\n        with open(bib_file, 'r', encoding='utf-8') as file:\n            affiliation_str = None\n            abstract_str = None\n            article_id = None\n            for line in file:\n                try:\n                    # Extract article ID using regex\n                    article_id = re.search('@[^{]+{(.*),', line).group(1)\n                    if article_id is None:\n                        print(f'ERROR: {line}')\n                        affiliation_str = None\n                        abstract_str = None\n                        continue\n                except:\n                    pass\n                # Extract abstract content\n                if 'abstract =' in line:\n                    abstract_str = re.search('{(.*)}', line).group(1)\n                # Extract affiliations content\n                elif 'affiliations =' in line:\n                    affiliation_str = re.search('{(.*)}', line).group(1)\n                # If both abstract and affiliation are found, add to the set\n                if abstract_str and affiliation_str:\n                    if article_id is None:\n                        print(f'ERROR: {abstract_str}')\n                    affiliation_set.add((article_id, affiliation_str, abstract_str))\n                    article_count += 1\n                    # Reset variables for the next article\n                    article_id = None\n                    affiliation_str = None\n                    abstract_str = None\n\n    # Load candidate labels from the abstract tag file\n    print('load candidate_labels')\n    print(f'article count: {article_count} vs {len(affiliation_set)}')\n    with open(args.abstract_tag_file, 'r') as file:\n        candidate_labels = ', '.join([v for v in file.read().split('\\n') if len(v) &gt; 0])\n    print(candidate_labels)\n\n    # Initialize the zero-shot classification pipeline\n    batch_size = 10\n    classifier = pipeline(\n        \"zero-shot-classification\",\n        model=\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\",\n        device=0,  # Use GPU if available\n        batch_size=batch_size,\n        truncation=True\n    )\n    print(classifier.model.device)\n\n    # Determine the target path for the output\n    target_path = args.target_path\n    if target_path is None:\n        target_path = '%s_classified%s' % os.path.splitext(args.affiliation_pickle_list)\n\n    total_time = 0\n    events = 0\n\n    # Open the target file for writing the classification results\n    with open(target_path, 'w', encoding='utf-8') as file:\n        print(f'opening {target_path} for writing {len(affiliation_set)} affiliations')\n        start_time = time.time()\n\n        # Generator function to yield affiliation strings\n        def affiliation_generator():\n            for _, affiliation_str, _ in affiliation_set:\n                yield affiliation_str\n\n        index = 1\n        # Perform classification and write results\n        for (article_id, affiliation_str, abstract_str), result in zip(\n            affiliation_set,\n            classifier(affiliation_generator(), candidate_labels, multi_label=True)\n        ):\n            file.write(f'{article_id}\\n{affiliation_str}\\n{abstract_str}\\n')\n            for label, score in zip(result['labels'], result['scores']):\n                file.write(f'{label}: {score}\\n')\n            file.write('\\n')\n            file.flush()\n            current_time = (time.time() - start_time)\n            events += 1\n            total_time += current_time\n            print(f'({index}/{len(affiliation_set)}) took {current_time}s to tag {article_id} '\n                  f'(time left: {total_time/events*(len(affiliation_set)-index)})')\n            start_time = time.time()\n            index += 1\n\nif __name__ == '__main__':\n    main()\n\n\nNot only does ChatGPT comment the code for me, it also gives an overview of the script at the end. I did not notice any changes to the code, except for the removal of some lines of code that had been commented out in the original (and were thus not functional). But it would be important to verify functionality of the updated code!\n\n\n\n\n\n\n\n\n\n\n2.3 Suggesting improvements to code (refactoring)\nOften we have a piece of code we’ve written, or borrowed from somewhere else, but it is slow or hard to read, and we want to improve it (refactoring). Perhaps we want to take a repeated process and streamline it or parallelize it across multiple processors. ChatGPT can help with this as well.\n\n\n\n\n\n\nExercise 8\n\n\n\n\n\n\nLet’s ask ChatGPT to take the nested for-loop above (a functional but not particularly elegant piece of code) and ask ChatGPT to recommend improvements.\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nBefore offering refactored code, it offered some tips, quoted here:\n\nYour code is functional, but it can be improved for efficiency and elegance. Here are some suggested improvements:\nKey Enhancements:\n\nAvoid explicit indexing (i &lt;- 1): Instead of manually managing an index for the list, you can use append() or lapply().\nUse split() + lapply(): This avoids explicit for loops and makes the code more concise and efficient.\nUse bind_rows() from dplyr: Instead of do.call(rbind, results), dplyr::bind_rows() is faster and more readable.\n\n\nNote that ChatGPT can’t see the rest of my script, including the part where I suggested bind_rows()…\nHere’s the refactored code it suggested:\n\nlibrary(dplyr)\n\n# Function to apply calc_lm across groups\nresults_df &lt;- penguins_filtered %&gt;%\n  split(list(.$species, .$sex), drop = TRUE) %&gt;%  # Split by species and sex\n  lapply(function(df) {\n    result &lt;- calc_lm(df, \"bill_length_mm\", \"bill_depth_mm\")  # Apply function\n    result$species &lt;- unique(df$species)  # Add species info\n    result$sex &lt;- unique(df$sex)          # Add sex info\n    return(result)\n  }) %&gt;%\n  bind_rows()  # Combine into a single dataframe\n\nThe results are functionally identical (though some unimportant differences in format - e.g., the order of rows, or the row names).\n\n\n\n\n\n\n\n\n\n\n2.4 Writing unit tests\nWhen writing functions, it is good practice to write unit tests to ensure that the function works as expected. A unit test is a bit of code that verifies the accuracy of a small, isolated block of application code (i.e., a small function). Typically they run a function with known inputs and test to make sure the outputs match expectations. ChatGPT can help you write unit tests for your functions, which can be especially helpful if you are not familiar with them.\n\n\n\n\n\n\nExercise 9\n\n\n\n\n\n\nLet’s ask ChatGPT to write a unit test for our calc_lm() function. This is a simple function that takes two vectors of data, runs a linear regression, and reports the slope, intercept, and p value.\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nHere is what ChatGPT suggested:\n\n\nCode\nlibrary(testthat)\n\n# Load the function (if not already sourced)\n# source(\"your_script.R\")\n\n# Create a test dataset\ntest_data &lt;- data.frame(\n  x = c(1, 2, 3, 4, 5),\n  y = c(2, 4, 6, 8, 10)  # Perfect linear relationship (y = 2x)\n)\n\ntest_that(\"calc_lm computes correct slope, intercept, and p-value\", {\n  result &lt;- calc_lm(test_data, \"y\", \"x\")\n  \n  # Expected values\n  expected_slope &lt;- 2  # Since y = 2x\n  expected_intercept &lt;- 0  # No intercept\n  expected_p_value &lt;- 2.2e-16  # Should be very close to zero due to perfect correlation\n  \n  # Check if slope and intercept are as expected\n  expect_equal(result$slope, expected_slope, tolerance = 1e-6)\n  expect_equal(result$intercept, expected_intercept, tolerance = 1e-6)\n  \n  # Check if p-value is very small (indicating strong statistical significance)\n  expect_lt(result$p_value, 0.05)\n})\n\n\nThis uses the testthat package, designed specifically for unit testing. It creates a test dataset with a perfect linear relationship, feeds it into the function, and compares the output values to our expectations. It is a good start, but when running it, there is an unexpected snag. The test doesn’t take into account that summary() on a perfectly linear relationship throws a warning, causing the unit test to fail. We could easily adjust the test to account for this, e.g., by adding a little noise into the test data to make it not perfectly linear, but the important thing is that we as the coder must verify that the code works as expected.\nJust to see what would happen, I told ChatGPT about the warning:\n\n\n\n\n\nAnd it adjusted the unit test to incorporate a small amount of noise, just as I would have done!\n\n\nCode\nlibrary(testthat)\n\n# Load the function (if not already sourced)\n# source(\"your_script.R\")\n\n# Create a test dataset with slight randomness to avoid perfect collinearity\nset.seed(123)  # For reproducibility\ntest_data &lt;- data.frame(\n  x = c(1, 2, 3, 4, 5),\n  y = c(2, 4, 6, 8, 10) + rnorm(5, mean = 0, sd = 0.1)  # Adding slight noise\n)\n\ntest_that(\"calc_lm computes correct slope, intercept, and p-value\", {\n  result &lt;- calc_lm(test_data, \"y\", \"x\")\n  \n  # Expected values (approximately)\n  expected_slope &lt;- 2\n  expected_intercept &lt;- 0\n  \n  # Check if slope and intercept are close to expected values\n  expect_equal(result$slope, expected_slope, tolerance = 0.1)\n  expect_equal(result$intercept, expected_intercept, tolerance = 0.1)\n  \n  # Check if p-value indicates strong statistical significance\n  expect_lt(result$p_value, 0.05)\n})\n\n\n\n\n\n\n\n\n\n\n\n\n2.5 Translating code from one language to another\nIf you are proficient in one language but not another, ChatGPT can help you translate code from one language to another. This can be especially helpful as you are learning a new language, or if some functionality available in an unfamiliar language is not available in your preferred language.\n\n\n\n\n\n\nExercise 10\n\n\n\n\n\n\nLet’s try giving ChatGPT the nested loop code again, but this time ask to translate to Python.\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nHere’s what it returned for me, nearly a line-by-line translation with a note: “Here’s the equivalent Python code using pandas and list comprehensions for efficiency.”\nHowever, note that the code is not stand-alone, since it calls the function calc_lm() that we defined previously but in R, and calls for the penguins_filtered object which has not yet been loaded and filter in Python. But those can be fixed pretty easily! (e.g., ask ChatGPT to rewrite calc_lm in Python, and suggest code for loading/filtering the penguins data in Python).\n\nimport pandas as pd\n\n# Create an empty list to store results\nresults = []\n\n# Iterate over species\nfor species in penguins_filtered['species'].unique():\n    # Iterate over sex\n    for sex in penguins_filtered['sex'].unique():\n        # Filter the DataFrame for the specific species and sex\n        subset = penguins_filtered[(penguins_filtered['species'] == species) & \n                                   (penguins_filtered['sex'] == sex)]\n        \n        # Apply the function to the filtered data\n        result = calc_lm(subset, \"bill_length_mm\", \"bill_depth_mm\")\n        \n        # Add species and sex information\n        result['species'] = species\n        result['sex'] = sex\n        \n        # Append the result to the list\n        results.append(result)\n\n# Combine the results into a single DataFrame\nresults_df = pd.concat(results, ignore_index=True)\n\nAfter the code, it listed some changes/improvements it made, as well, which is helpful.\n\n\n\n\nAnd as long as we’re here, let’s ask it to refactor this code in Python instead of R:\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nAnd again, it provided a little context, and some nice tidy code (with the same limitations noted above). Note this refactored version in Python looks pretty different from the refactored version in R, but shares some functional similarities.\n\nimport pandas as pd\n\n# Define a function to process each group\ndef process_group(group):\n    result = calc_lm(group, \"bill_length_mm\", \"bill_depth_mm\")  # Apply function\n    result[\"species\"] = group[\"species\"].iloc[0]  # Assign species\n    result[\"sex\"] = group[\"sex\"].iloc[0]  # Assign sex\n    return result\n\n# Apply function across groups and combine results\nresults_df = pd.concat(\n    [process_group(group) for _, group in penguins_filtered.groupby([\"species\", \"sex\"])],\n    ignore_index=True\n)\n\n\n\n\n\n\n\n\n\n\n\n2.6 Summary: ChatGPT for coding\n\n\n\n\n\n\nSummary: ChatGPT tips\n\n\n\n\nChatGPT can provide coding support by:\n\ngenerating code from pseudocode\ncommenting existing code\nsuggesting improvements to code (refactoring)\ntranslating code from one language to another\n\nAs with Copilot, when you provide more context to ChatGPT (as pseudocode, example code, desired packages, etc), you will get better results\nAs with Copilot, you are responsible for the final product. Test the code, check for errors, and make sure it works as expected!",
    "crumbs": [
      "AI for Coding Support"
    ]
  },
  {
    "objectID": "s01_ai_coding_support.html#further-considerations",
    "href": "s01_ai_coding_support.html#further-considerations",
    "title": "AI for Coding Support",
    "section": "3 Further considerations",
    "text": "3 Further considerations\nWhat about hallucinations and errors?\n\nLLMs and generative AI are known to “hallucinate” and make up incorrect information from time to time. In coding, this may be less of an issue, since you can run the code to test it and verify that the results match expectations.\n\nHowever, if you are asking an LLM to code something that you don’t fully understand, you may not recognize errors or incorrect results. It might be risky to ask an LLM to write code for you that you wouldn’t be able to write yourself if given the time.\nLLMs are essentially trying to provide the most likely completion to a given prompt, i.e., the least surprising answer. For creative writing tasks, this can result in pretty mediocre and predictable prose. But for a highly structured task like coding, surprise is not the goal! An LLM trained on a massive code base will have a pretty good idea of the most common - and least surprising - ways to approach a typical coding problem.\n\nWhat about ethics?\n\nLLMs and generative AI are trained on large datasets, often including content scraped from artists and writers (and coders) without their consent. This raises ethical concerns about the use of generative AI for creative tasks, for plagiarism and exploitation of writers’ work and for the potential for putting artists out of work. For coding tasks this may be less of an issue, as much of the codebase is open source, e.g., public repos on Github or Q&A on Stack Overflow, though even open source material is often licensed with rules for attribution that may be violated by AI-generated code.\nWhile an LLM doesn’t ingest the data or information you put into your prompts (once it is trained, it can’t learn new things), it is still important to consider security and privacy risks. Including sensitive or private information in a prompt may result in some of that sensitive information being exposed in the resulting code. If you are coding an analysis that requires privacy or security, the LLM may not properly understand those privacy or security concerns and could generate code with security liabilities.",
    "crumbs": [
      "AI for Coding Support"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About the course",
    "section": "",
    "text": "AI for NCEAS",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#nceas-expertise",
    "href": "index.html#nceas-expertise",
    "title": "About the course",
    "section": "1 NCEAS Expertise",
    "text": "1 NCEAS Expertise\nThe National Center for Ecological Analysis and Synthesis (NCEAS), a research affiliate of UCSB, is a leading expert on interdisciplinary data science and works collaboratively to answer the world’s largest and most complex questions. The NCEAS approach leverages existing data and employs a team science philosophy to squeeze out all potential insights and solutions efficiently - this is called synthesis science.\nNCEAS has over 25 years of success with this model among working groups and environmental professionals. Together with the Delta Science Program and the Delta Stewardship Council we are excited to pass along skills, workflows, mindsets learn throughout the years.\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nEffectively manage data using tidy data practices\nImplement reproducible scientific workflows throughout all aspects of a project\nEstablish best practices and utilize tools like Git & GitHub to optimize your collaboration\nBetter communicate scientific analyses and results using Markdown, GitHub webpages, and R packages like ggplot2 and Shiny\nIncrease your familiarity and confidence with data science tools\n\n\n\n\n1.1 Week’s Schedule",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "About the course",
    "section": "2 Code of Conduct",
    "text": "2 Code of Conduct\nBy participating in this activity you agree to abide by the NCEAS Code of Conduct.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "About the course",
    "section": "3 About this book",
    "text": "3 About this book\nThese written materials are the result of a continuous and collaborative effort at NCEAS to help researchers make their work more transparent and reproducible. This work began in the early 2000’s, and reflects the expertise and diligence of many, many individuals. The primary authors are listed in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCitation: Camila Vargas Poulsen & Angel Chen (2024), NCEAS coreR for Delta Science Program, June 2024, NCEAS Learning Hub. URL https://learning.nceas.ucsb.edu/2024-06-delta.\nAdditional contributors: Ben Bolker, Amber E. Budden, Julien Brun, Samantha Csik, Halina Do-Linh, Natasha Haycock-Chavez, S. Jeanette Clark, Julie Lowndes, Stephanie Hampton, Matt Jone, Samanta Katz, Erin McLean, Bryce Mecum, Deanna Pennington, Karthik Ram, Jim Regetz, Tracy Teal, Daphne Virlar-Knight, Leah Wasser.\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "s02_ai_and_apis.html",
    "href": "s02_ai_and_apis.html",
    "title": "Git and GitHub",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nApply the principles of Git to track and manage changes of a project\nUtilize the Git workflow including pulling changes, staging modified files, committing changes, pulling again to incorporate remote changes, and pushing changes to a remote repository\nCreate and configure Git repositories using different workflows",
    "crumbs": [
      "Git and GitHub"
    ]
  },
  {
    "objectID": "s02_ai_and_apis.html#introduction-to-version-control",
    "href": "s02_ai_and_apis.html#introduction-to-version-control",
    "title": "Git and GitHub",
    "section": "1 Introduction to Version Control",
    "text": "1 Introduction to Version Control",
    "crumbs": [
      "Git and GitHub"
    ]
  }
]