[
  {
    "objectID": "slides/ai_coding_support/slides.html#title-slide",
    "href": "slides/ai_coding_support/slides.html#title-slide",
    "title": "AI for NCEAS",
    "section": "",
    "text": "AI for coding support\nAn introduction to using AI tools to boost your coding practice"
  },
  {
    "objectID": "slides/ai_coding_support/slides.html#r-rstudio",
    "href": "slides/ai_coding_support/slides.html#r-rstudio",
    "title": "AI for NCEAS",
    "section": "",
    "text": "Configure RStudio to work with Copilot\n\n\nIn RStudio: Tools \\(\\Rightarrow\\) Global Options. Look for “Copilot” at the bottom of the sidebar*:\n\n\n\n\n\n* Copilot requires RStudio version 2023.09.0 or later!"
  },
  {
    "objectID": "s02_ai_analysis_in_llm.html",
    "href": "s02_ai_analysis_in_llm.html",
    "title": "Data Inspection and Analysis using LLMs",
    "section": "",
    "text": "Learning Objectives\n\n\n\nBy the end of this session, participants will be able to use ChatGPT effectively to inspect, explore, and generate insights from datasets, and to accelerate common analytical workflows through prompt engineering.\n\nDataset Inspection\n\nexamine missing values\ncalculate summary statistics\ntransform data\n\nStatistical Analysis\n\ngenerate time series plots\nidentify potential analytical directions\ncreate report-ready narratives using natural language generation\nexport code for reproducibility",
    "crumbs": [
      "Data Inspection and Analysis using LLMs"
    ]
  },
  {
    "objectID": "s02_ai_analysis_in_llm.html#who-and-why-to-use-chatgpt-for-data-analysis",
    "href": "s02_ai_analysis_in_llm.html#who-and-why-to-use-chatgpt-for-data-analysis",
    "title": "Data Inspection and Analysis using LLMs",
    "section": "1 Who and why to use ChatGPT for data analysis?",
    "text": "1 Who and why to use ChatGPT for data analysis?\n\nIf you’re not a programmer or statistician, ChatGPT can help you understand the overall story of your dataset by breaking down key patterns, trends, and issues in plain language.\nIf you’re working on a data synthesis project and need to filter through multiple datasets, ChatGPT can assist in rapidly exploring each dataset—using visualizations and summaries—to help you decide which ones are most relevant for your goals.\nIf you want to apply statistical methods but don’t recall all the details, ChatGPT can suggest appropriate techniques based on your data and research questions, and generate ready-to-use code for implementation in Python or R.\nIf you need to produce a quick data report, ChatGPT can significantly speed up the process by generating summary statistics, visualizations, and clear written narratives tailored for stakeholders or publication.",
    "crumbs": [
      "Data Inspection and Analysis using LLMs"
    ]
  },
  {
    "objectID": "s02_ai_analysis_in_llm.html#dataset-inspection",
    "href": "s02_ai_analysis_in_llm.html#dataset-inspection",
    "title": "Data Inspection and Analysis using LLMs",
    "section": "2 Dataset Inspection",
    "text": "2 Dataset Inspection\n\nUploading tabular data directly from your computer is recommended, as it ensures ChatGPT has access to the exact dataset you intend to analyze. Sharing a link may not always work reliably, especially if the data is behind access restrictions or changes dynamically.\nLooking for a dataset to practice with? Try the SBCLTER annual fish survey. For free version of the ChatGPT, try dataset that is smaller than 10 KB, such as Subset version of the annual fish survey.\nIn each section below, the example prompts are provided. You can copy and paste it into ChatGPT, or type your own prompts.\n\n\n\n\n\n\n\nChatGPT prompt\n\n\n\n\nTell me about this dataset, and what types of data does it contain?\nWhich columns are affected by the missing values and how many missing values are in each column.\nProvide summary statistics (mean, median, std, min, max) for count and size column, and unique values for the site, and year columns.\nConvert this data table to wide format using sp_code as header and count as value",
    "crumbs": [
      "Data Inspection and Analysis using LLMs"
    ]
  },
  {
    "objectID": "s02_ai_analysis_in_llm.html#statistical-analysis",
    "href": "s02_ai_analysis_in_llm.html#statistical-analysis",
    "title": "Data Inspection and Analysis using LLMs",
    "section": "3 Statistical Analysis",
    "text": "3 Statistical Analysis\n\n\n\n\n\n\nChatGPT prompt\n\n\n\na. Time series plot\n\nCreate a time series plot of the total species count, with year on the x-axis and count on the y-axis. Use different colors for each site.\nCreate a time series plot of the top 5 most abundant species, with year on the x-axis and count on the y-axis. Use different colors for each site.\n\nb. Identify Potential Analytical Directions\n\nI want to understand whether species has an increasing trend over time\nI like to do a glm() to see the species trend over time, which family should I use? how about the link function?\n\nc. Create Report-Ready Narratives\n\nI like this analysis and want to document in the report. Please describe statistical analysis for the glm() model and the corresponding results\n\nd. Export Code for Reproducibility\n\nPlease give me the R scripts for the glm() analysis\n\n\n\n\n\n\n\n\n\nImportant Notes\n\n\n\n\nChatGPT is not a substitute for human expertise, but it serves as a powerful assistant that can enhance and accelerate your analytical workflow.\nData analysis done within ChatGPT is not reproducible. If you discover valuable insights, always ask ChatGPT to generate the underlying code so you can document and reproduce the process independently.\nUsing a customized GPT model is recommended for data analysis, as it can handle larger datasets and provide more context-aware responses. Therefore, access to the paid version of ChatGPT (such as ChatGPT Plus or Team) is required for this exercise.\nIf you’re comparing the speed of conducting analysis in ChatGPT versus R (with existing code), R is generally more reliable and consistently faster. ChatGPT’s performance can vary depending on the time of day, server load, and the specific GPT model you’re using.\n\n\nAs of April 2025, here are the estimated CSV table size by GPT Model\n\n\n\n\n\n\n\n\n\nModel\nToken Limit\nApprox. Rows × Columns (Typical CSV)\nNotes\n\n\n\n\nGPT-3.5\n4,096\n~200 rows × 10 columns\nOnly good for small previews or subsets\n\n\nGPT-4 (8k)\n8,192\n~400 rows × 10 columns\nModerate analysis and EDA possible\n\n\nGPT-4-turbo (128k)\n128,000\n~6,000–10,000 rows × 10–20 columns\nSupports large datasets and full analysis\n\n\n\n\nEach cell ≈ 5–10 tokens (e.g., a short string or number).",
    "crumbs": [
      "Data Inspection and Analysis using LLMs"
    ]
  },
  {
    "objectID": "s01_ai_coding_support.html",
    "href": "s01_ai_coding_support.html",
    "title": "AI for Coding Support",
    "section": "",
    "text": "Learning objectives\n\n\n\nBy the end of this session, you should be able to:\n\nEffectively use Github Copilot in RStudio:\n\nConfigure RStudio to work with GitHub Copilot\nUse comments and pseudocode to guide Copilot to assist with coding tasks\n\nLeverage ChatGPT (or another Large Language Model) to assist with coding tasks:\n\nTranslate pseudocode into functional code\nComment existing code\nSuggest improvements to code for performance or readability\nWrite unit tests for functions\nTranslate code from one language to another (e.g., R vs. Python)\nWe can use GitHub Copilot and/or Large Language Models like ChatGPT or Claude to help us write code in R, Python, or other languages, using natural language such as English. It might be handy to consider these as two different avenues of leveraging AI in coding:",
    "crumbs": [
      "AI for Coding Support"
    ]
  },
  {
    "objectID": "s01_ai_coding_support.html#active-coding-support-with-github-copilot",
    "href": "s01_ai_coding_support.html#active-coding-support-with-github-copilot",
    "title": "AI for Coding Support",
    "section": "1 Active coding support with GitHub Copilot",
    "text": "1 Active coding support with GitHub Copilot\nGitHub Copilot is an AI-powered code completion tool that helps you write code faster and with fewer errors. It is effectively “pair programming” where you and a partner (in this case, an AI partner) code together in real time. It is available as a plugin for IDEs such as Visual Studio Code and RStudio. Copilot is a large language model (LLM) related to ChatGPT, trained on GitHub and other codebases, and it can generate code completions in real time based on the context of your coding task. In this lesson, we will configure RStudio to work with GitHub Copilot and practice techniques to use Copilot effectively to assist with coding tasks.\nIf you have an education GitHub account (e.g., an .edu email address that is associated with your GitHub account), you should be able to get free access to Copilot Pro. Otherwise you can access a free but limited version of Copilot, which should be adequate for this session. See more Copilot requirements here.\n\n\n\n\n\n\nNote\n\n\n\nHere we will focus on using AI code completion tools in R and RStudio, since it is a popular language and IDE for ecologists, but the same methods work for other languages and IDEs. VS Code is a Microsoft product, as is Copilot (and GitHub for that matter), so its integration is even better than RStudio.\n\n\n\n1.1 Configuring RStudio to work with GitHub Copilot\nFull Screen\n\n\n\n1.2 Using Copilot to assist with coding tasks\nYou need a fundamental understanding of coding to be able to use Copilot effectively. It can make intelligent guesses as to your intentions, but doesn’t necessarily know where your files are, or what you intend to do with the data. So it is helpful to guide Copilot in the right direction using comments, sentences, and “pseudocode” before you start any actual coding. The more information you provide through your Markdown text and code-chunk comments, the better Copilot will be able to help you.\n\n\n\n\n\n\nPseudocode\n\n\n\nPseudocode is a way of planning out an analysis by writing out the sequence of steps you expect to take, in plain language, perhaps as bullet points. There are more formalized versions of pseudocode, and less formalized (see “vibecoding”) but here we take a middle road, and we will stick with just plain English sentences to communicate our intentions. Some tips:\n\nSpecify whether you want to use the tidyverse metapackage or base R functions (or another language, e.g., Python!).\nIf you know the names of certain functions, it is helpful to use those in your pseudocode, to help Copilot incorporate those into its code completions.\nUse comments to signal to Copilot what you’d like to do at each step.\n\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\n\n\nCreate a code chunk, read in some data, do some data wrangling, and then make a plot with the resulting data. Use comments in your code chunk to signal to Copilot what you’d like to do at each step. Here are the steps we will do, using a tidyverse workflow:\n\nload the palmerpenguins package and load the penguins data\ndrop any incomplete observations (any NA values)\nfilter to just keep the Chinstrap and Adelie species\ngenerate a scatter plot of bill length vs. bill depth for each species\nthen, for the two species, run a t-test to determine whether the bill lengths are statistically different between the two species. Print a summary of the t test results.\n\n(Copy the above text and paste into a Quarto document, or rephrase it in your own words, then create a code chunk and let Copilot suggest some code!)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nHere’s what it looked like when I let Copilot help me write the code - I accepted each line by pressing “tab”. This might change entirely if I ran the exercise again.\n\nHow different does it look from the code you wrote?\nHow does it differ from how you might code it on your own?\n\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\nWarning: package 'stringr' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWarning: package 'palmerpenguins' was built under R version 4.3.3\n\n\n\n\n\n\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  bill_length_mm by species\nt = -21.712, df = 108.17, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Adelie and group Chinstrap is not equal to 0\n95 percent confidence interval:\n -10.92365  -9.09605\nsample estimates:\n   mean in group Adelie mean in group Chinstrap \n               38.82397                48.83382 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nestimate\nestimate1\nestimate2\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n-10.00985\n38.82397\n48.83382\n-21.7125\n0\n108.1722\n-10.92365\n-9.09605\nWelch Two Sample t-test\ntwo.sided\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\n\n\nLet’s try the same exercise but this time let’s only use base R functions:\n\nload the palmerpenguins package and load the penguins data\ndrop incomplete observations\nfilter to just keep the Chinstrap and Adelie species\ngenerate a scatter plot of bill length vs. bill depth for each species\nthen, for the two species, run a t-test to determine whether the bill lengths are statistically different between the two species. Print a summary of the t test results.\n\n(Below the previous code chunk, in a Markdown section, copy and paste the above text, or simply write a sentence along the lines of “Repeat the previous analysis using only base R functions and plotting.” Then create a new code chunk and let Copilot suggest code for you.)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nHere is what Copilot came up with for me. Note, it knew from above that I wanted to use base R functions this time instead of tidyverse functions!\n\n\n\n\n\n\n\n\n\n            Length Class  Mode     \nstatistic   1      -none- numeric  \nparameter   1      -none- numeric  \np.value     1      -none- numeric  \nconf.int    2      -none- numeric  \nestimate    2      -none- numeric  \nnull.value  1      -none- numeric  \nstderr      1      -none- numeric  \nalternative 1      -none- character\nmethod      1      -none- character\ndata.name   1      -none- character\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCopilot limitations!\n\n\n\nCopilot cannot read your mind, and it may make incorrect assumptions about what you want to do with your data and analysis. It is important to understand the code that Copilot is generating for you, and to check it for errors!\n\nCopilot can only add code or text; it cannot subtract or edit buggy or incorrect code or text without specific prompting from the user.\nYou can always reject Copilot’s suggestions and write your own code, or accept its suggestions as a starting point then edit them as you see fit before moving on.\nYou get the last word (no matter how hard Copilot tries!), and you as the coder are responsible for the final product.\n\n\n\n\n\n1.3 Using Copilot for more complex coding tasks\nCopilot can also help you write functions and loops, among other things.\n\n1.3.1 Copilot to write a function\n\n\n\n\n\n\nExercise 3\n\n\n\n\n\n\nLet’s write a function that takes a dataframe of data and for two given columns, calculates the slope, intercept, and p value for a linear regression model. Let’s call it calc_lm(). Some pseudocode to get you started:\n\nidentify and pull out the y (outcome) and x (predictor) variables as columns in the data frame\nperform a linear regression on y ~ x\nreturn the slope, intercept, and p value in a dataframe.\n\n(Feel free to copy the above text and paste into a Markdown section of your Quarto document, then let Copilot make its suggestions)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nHere’s what Copilot suggested for me, line by line, as I was writing this lesson. After finishing the function, I started writing a comment to test, it assumed I wanted to use data that I had mentioned earlier in the script, i.e., penguins_filtered.\n# function to calculate slope, intercept, and p value for a linear regression model\ncalc_lm &lt;- function(data, y, x) {\n  # identify and pull out the y (outcome) and x (predictor) variables as columns in the data frame\n  y &lt;- data[[y]]\n  x &lt;- data[[x]]\n  \n  # perform a linear regression on y ~ x\n  lm_model &lt;- lm(y ~ x, data = data)\n  \n  # return the slope, intercept, and p value in a dataframe\n  data.frame(slope = coef(lm_model)[2],\n             intercept = coef(lm_model)[1],\n             p_value = summary(lm_model)$coefficients[2, 4])\n}\n### test on the penguins data\ncalc_lm(penguins_filtered, \"bill_length_mm\", \"bill_depth_mm\")\nDoes Copilot’s suggestion work for you? If not take a moment to fix it and make it work. For me, it created a perfectly good function the first time. Some observations on the code presented here:\n\nI would have avoided overwriting the local value of x and y in the function, as it could be confusing.\nNote that the p value it selects from the linear model is that of the slope, not the intercept. That is probably what we wanted, but would be a good idea to make sure that meets our expectations!\nI might also prefer to have the output in a different format, perhaps using broom::tidy().\n\n\n\n\n\n\n\n\n\n\n\n1.3.2 Copilot to write a loop\n\n\n\n\n\n\nExercise 4\n\n\n\n\n\n\nNow let’s get Copilot to help use the previous function to iterate over the various species by sex. Using for-loops, this will involve nested loops (one for species, one for sex), so let’s see if Copilot can figure that out! Pseudocode:\n\nFor each species, and each sex, apply the function above to the penguins data, and add the species and sex to the resulting dataframe.\nStore the results in a list of dataframes\nThen combine the results into a single dataframe.\n\n(Again, copy the text above and paste into a Markdown section, open a new code chunk and let Copilot make suggestions. It may struggle with this, so if it gets stuck, help it by writing some code or comments)\n\n\n\n\n\n\nWarning\n\n\n\nI almost certainly wouldn’t have actually used a for loop here in the first place - better options would be a group_by() %&gt;% summarize() workflow or purrr::map2 - but we’re just trying to see what Copilot can do with a more complex coding task!\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nHere’s what Copilot suggested, based on the pseudocode and previous code chunks. However, this was a little more complex and required some guidance, hand holding, and bug-fixing.\n# create an empty list to store the results\n1results &lt;- list()\n2i &lt;- 1\n\n# iterate over the species\nfor (species in unique(penguins_filtered$species)) {\n  # iterate over the sex\n3  for(sex in unique(penguins_filtered$sex)) {\n    # apply the function above to the penguins data\n4    result &lt;- calc_lm(penguins_filtered[penguins_filtered$species == species & penguins_filtered$sex == sex, ], \"bill_length_mm\", \"bill_depth_mm\")\n    # add the species and sex to the resulting dataframe\n    result$species &lt;- species\n5    result$sex &lt;- sex\n    # store the results in the list\n6    results[[i]] &lt;- result\n7    i &lt;- i + 1\n  }\n}\n\n# combine the results into a single dataframe\nresults_df &lt;- do.call(rbind, results)\n  \n\n1\n\nCopilot didn’t pre-allocate space for the list based on the number of expected entries. Not a dealbreaker for a small project, but generally a good idea.\n\n2\n\nAfter Copilot started the loop, it didn’t include an indexing variable, and hadn’t considered that the “species” index would be character, not numeric, so would be challenging to use as an indexing variable after the fact. I manually added in an index i to keep track of the list index.\n\n3\n\nCopilot struggled with this nested loop format - so I had to manually start typing this line; but then Copilot was able to finish.\n\n4\n\nAgain, Copilot struggled here - it wrote the function call halfway through the filtered step; when I wrote the penguins_filtered$sex == sex part, it was able to finish the line. However, it made some errors finishing the line as well!\n\n5\n\nOnce again, Copilot wasn’t sure how to add the sex info to the resulting dataframe, so I had to manually start writing.\n\n6\n\nHere, Copilot used a single bracket for indexing, which is incorrect for a list in R. I had to manually change this to double brackets.\n\n7\n\nCopilot didn’t include an incrementing index variable, so I had to manually add this.\n\n\nIn summary, Copilot was helpful in getting the ball rolling here, but needed lots of manual help. Additionally, it made some choices I probably would have made differently - e.g., I probably would have filtered the penguins_filtered dataframe before the calc_lm call, and probably would have used dplyr::mutate to add the species and sex columns to the result. Finally, I probably would have used bind_rows() instead of do.call(rbind, results) to combine the results, but that’s just what I’m used to.\n\n\n\n\n\n\n\n\n\n\n\n1.4 Copilot for coding: summary\nCopilot can be very helpful at speeding up your coding, especially for simple or well-defined tasks. One potential pitfall is that using Copilot in this way has a certain amount of path dependency. Once a bit of code is written in the script, Copilot takes that as given - it can’t modify or erase existing code, it can only keep going forward by adding.\nIn the next section, we’ll look at how to use ChatGPT to help with coding in a more interactive/iterative fashion, which can avoid path dependency but may introduce other complexities.\n\n\n\n\n\n\nSummary: Copilot tips\n\n\n\n\nBefore starting your analysis, prime Copilot by writing out an overview of your analysis, including packages, functions, and statistical tests you want to do.\nWhile writing code, if Copilot gets stuck, or treads down an unproductive path, you have several options:\n\nwrite out a comment or additional pseudocode to guide Copilot in the right direction.\naccept Copilot’s suggestion (if it’s on the right track) and manually fix it before continuing.\nreject Copilot’s suggestion (if it’s way off base) and write your own code, to signal what you really want\n\nRun the code to make sure it works as expected, and check for errors. You are responsible for the final result.\nRemember, you can turn Copilot off entirely, or set it to autocomplete only when requested:\n\nTools –&gt; Global Options –&gt; Copilot –&gt; Show Copilot code suggestions: Manually (Ctrl-Backslash)).",
    "crumbs": [
      "AI for Coding Support"
    ]
  },
  {
    "objectID": "s01_ai_coding_support.html#iterative-coding-support-with-chatgpt",
    "href": "s01_ai_coding_support.html#iterative-coding-support-with-chatgpt",
    "title": "AI for Coding Support",
    "section": "2 Iterative coding support with ChatGPT",
    "text": "2 Iterative coding support with ChatGPT\nIn this section we’ll look at using ChatGPT outside of RStudio. Here we can take existing code and feed it into ChatGPT and ask it to help us with various tasks, or feed pseudocode into ChatGPT and ask it to generate code for us. Rather than line by line, like Copilot, an LLM can often provide a more complete solution that can then be copied/pasted back into RStudio. It can also iterate through multiple rounds of conversation/prompts.\n\n2.1 Pseudocode into code\nJust as Copilot can generate code from pseudocode, ChatGPT can do the same. You can then copy/paste the results into RStudio.\n\n\n\n\n\n\nExercise 5\n\n\n\n\n\n\nLet’s give ChatGPT the same sequence of steps we used above. We will copy and paste the pseudocode from above (repeated here) into ChatGPT and ask it to generate code for us.\n\nload the palmerpenguins package and load the penguins data\ndrop any incomplete observations (any NA values)\nfilter to just keep the Chinstrap and Adelie species\ngenerate a scatter plot of bill length vs. bill depth for each species\nthen, for the two species, run a t-test to determine whether the bill lengths are statistically different between the two species. Print a summary of the t test results.\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nHere is the code it generated for me. Note it chose a (mostly) tidyverse approach this time, though I didn’t explicitly prompt it to do so:\n# Load necessary packages\nlibrary(palmerpenguins)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Load the penguins dataset and remove incomplete observations\npenguins_clean &lt;- penguins %&gt;%\n  na.omit() %&gt;%\n  filter(species %in% c(\"Chinstrap\", \"Adelie\"))\n\n# Generate scatter plot of bill length vs. bill depth\nggplot(penguins_clean, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point() +\n  labs(title = \"Bill Length vs. Bill Depth\",\n       x = \"Bill Length (mm)\",\n       y = \"Bill Depth (mm)\") +\n  theme_minimal()\n\n# Perform a t-test on bill length between Chinstrap and Adelie species\nt_test_result &lt;- t.test(bill_length_mm ~ species, data = penguins_clean)\n\n# Print summary of t-test results\nprint(t_test_result)\nThis code works exactly as desired, though we set it to not evaluate here in the script, for sake of space.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 6\n\n\n\n\n\n\nI could follow up with a request to not use any tidyverse packages…\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nAnd this is what it came up with:\n# Load the palmerpenguins package\nlibrary(palmerpenguins)\n\n# Load the dataset and remove incomplete observations\npenguins_clean &lt;- na.omit(penguins)\n\n# Keep only Chinstrap and Adelie species\npenguins_clean &lt;- penguins_clean[penguins_clean$species %in% c(\"Chinstrap\", \"Adelie\"), ]\n\n# Generate scatter plot of flipper length vs. bill depth\nplot(penguins_clean$bill_length_mm, penguins_clean$bill_depth_mm,\n     col = ifelse(penguins_clean$species == \"Chinstrap\", \"blue\", \"red\"),\n     pch = 19,\n     xlab = \"Bill Length (mm)\",\n     ylab = \"Bill Depth (mm)\",\n     main = \"Bill Length vs. Bill Depth\")\nlegend(\"topright\", legend = c(\"Chinstrap\", \"Adelie\"), col = c(\"blue\", \"red\"), pch = 19)\n\n# Perform a t-test on flipper length between Chinstrap and Adelie species\nt_test_result &lt;- t.test(bill_length_mm ~ species, data = penguins_clean)\n\n# Print summary of t-test results\nprint(t_test_result)\nThis code works exactly as desired, though we set it to not evaluate here in the script, for sake of space.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2 Commenting existing code\nOften when we are trying to code, we Google to find a solution online (e.g., Stack Overflow), but the code is not familiar to us or difficult to understand. ChatGPT can take that code and insert helpful comments to explain what each line is doing.\n\n\n\n\n\n\nExercise 7\n\n\n\n\n\n\nHere is a complex Python script that would be challenging to understand for someone not proficient in Python:\nhttps://raw.githubusercontent.com/convei-wwf/NLParty/refs/heads/main/abstract_classifier.py\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nHere’s what it came up with! Try asking ChatGPT to comment for you as if you were completely new to Python and see how it changes.\n#| eval: false\n\nimport argparse  # For parsing command-line arguments\nimport glob  # For file pattern matching\nimport logging  # For logging messages\nimport os  # For operating system interactions\nimport time  # For measuring execution time\nimport re  # For regular expressions\nimport warnings  # For handling warnings\nfrom transformers import pipeline  # For using Hugging Face's transformers pipeline\nimport torch  # For PyTorch operations\n\n# Suppress specific UserWarnings from torch.utils.data\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.utils.data\")\n\n# Configure logging settings\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format=(\n        '%(asctime)s (%(relativeCreated)d) %(levelname)s %(name)s'\n        ' [%(funcName)s:%(lineno)d] %(message)s'\n    )\n)\nlogging.getLogger('taskgraph').setLevel(logging.INFO)\nLOGGER = logging.getLogger(__name__)\n\ndef main():\n    # Check if CUDA (GPU support) is available and print the result\n    print(torch.cuda.is_available())\n\n    # Set up argument parser for command-line inputs\n    parser = argparse.ArgumentParser(description='Affiliation classifier')\n    parser.add_argument('bib_file_pattern', help='path to bibliography list')\n    parser.add_argument('abstract_tag_file', help='path to abstract tags')\n    parser.add_argument('--target_path', help='target classified table')\n    args = parser.parse_args()\n\n    # Load bibliography files matching the given pattern\n    print('load bib_file')\n    affiliation_set = set()\n    article_count = 0\n    for bib_file in glob.glob(args.bib_file_pattern):\n        with open(bib_file, 'r', encoding='utf-8') as file:\n            affiliation_str = None\n            abstract_str = None\n            article_id = None\n            for line in file:\n                try:\n                    # Extract article ID using regex\n                    article_id = re.search('@[^{]+{(.*),', line).group(1)\n                    if article_id is None:\n                        print(f'ERROR: {line}')\n                        affiliation_str = None\n                        abstract_str = None\n                        continue\n                except:\n                    pass\n                # Extract abstract content\n                if 'abstract =' in line:\n                    abstract_str = re.search('{(.*)}', line).group(1)\n                # Extract affiliations content\n                elif 'affiliations =' in line:\n                    affiliation_str = re.search('{(.*)}', line).group(1)\n                # If both abstract and affiliation are found, add to the set\n                if abstract_str and affiliation_str:\n                    if article_id is None:\n                        print(f'ERROR: {abstract_str}')\n                    affiliation_set.add((article_id, affiliation_str, abstract_str))\n                    article_count += 1\n                    # Reset variables for the next article\n                    article_id = None\n                    affiliation_str = None\n                    abstract_str = None\n\n    # Load candidate labels from the abstract tag file\n    print('load candidate_labels')\n    print(f'article count: {article_count} vs {len(affiliation_set)}')\n    with open(args.abstract_tag_file, 'r') as file:\n        candidate_labels = ', '.join([v for v in file.read().split('\\n') if len(v) &gt; 0])\n    print(candidate_labels)\n\n    # Initialize the zero-shot classification pipeline\n    batch_size = 10\n    classifier = pipeline(\n        \"zero-shot-classification\",\n        model=\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\",\n        device=0,  # Use GPU if available\n        batch_size=batch_size,\n        truncation=True\n    )\n    print(classifier.model.device)\n\n    # Determine the target path for the output\n    target_path = args.target_path\n    if target_path is None:\n        target_path = '%s_classified%s' % os.path.splitext(args.affiliation_pickle_list)\n\n    total_time = 0\n    events = 0\n\n    # Open the target file for writing the classification results\n    with open(target_path, 'w', encoding='utf-8') as file:\n        print(f'opening {target_path} for writing {len(affiliation_set)} affiliations')\n        start_time = time.time()\n\n        # Generator function to yield affiliation strings\n        def affiliation_generator():\n            for _, affiliation_str, _ in affiliation_set:\n                yield affiliation_str\n\n        index = 1\n        # Perform classification and write results\n        for (article_id, affiliation_str, abstract_str), result in zip(\n            affiliation_set,\n            classifier(affiliation_generator(), candidate_labels, multi_label=True)\n        ):\n            file.write(f'{article_id}\\n{affiliation_str}\\n{abstract_str}\\n')\n            for label, score in zip(result['labels'], result['scores']):\n                file.write(f'{label}: {score}\\n')\n            file.write('\\n')\n            file.flush()\n            current_time = (time.time() - start_time)\n            events += 1\n            total_time += current_time\n            print(f'({index}/{len(affiliation_set)}) took {current_time}s to tag {article_id} '\n                  f'(time left: {total_time/events*(len(affiliation_set)-index)})')\n            start_time = time.time()\n            index += 1\n\nif __name__ == '__main__':\n    main()\nNot only does ChatGPT comment the code for me, it also gives an overview of the script at the end. I did not notice any changes to the code, except for the removal of some lines of code that had been commented out in the original (and were thus not functional). But it would be important to verify functionality of the updated code!\n\n\n\n\n\n\n\n\n\n\n2.3 Suggesting improvements to code (refactoring)\nOften we have a piece of code we’ve written, or borrowed from somewhere else, but it is slow or hard to read, and we want to improve it (“refactoring”). Perhaps we want to take a repeated process and streamline it or parallelize it across multiple processors. ChatGPT can help with this as well.\n\n\n\n\n\n\nStart a new chat or continue with existing chat?\n\n\n\nSince we’ve already chatted with the LLM, if we continue in the same conversation it will “remember” the previous context. But perhaps we want to start fresh on a new problem.\n\nFor example, if I asked ChatGPT to comment the previous code as if I were a novice in Python, it may continue to assume I’m a novice for future questions, which may produce overly simplified or verbose responses. This might call for a new chat.\nAlternately, if I tried a bit of suggested code and encountered an error, I can paste that error back into the same conversation and ask ChatGPT to alter its suggestion accordingly. This would not be effective if I started a new chat.\n\n\n\n\n\n\n\n\n\nExercise 8\n\n\n\n\n\n\nLet’s ask ChatGPT to take the nested for-loop above (a functional but not particularly elegant piece of code) and ask ChatGPT to recommend improvements.\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nBefore offering refactored code, it offered some tips, quoted here:\n\nYour code is functional, but it can be improved for efficiency and elegance. Here are some suggested improvements:\nKey Enhancements:\n\nAvoid explicit indexing (i &lt;- 1): Instead of manually managing an index for the list, you can use append() or lapply().\nUse split() + lapply(): This avoids explicit for loops and makes the code more concise and efficient.\nUse bind_rows() from dplyr: Instead of do.call(rbind, results), dplyr::bind_rows() is faster and more readable.\n\n\nNote that I didn’t show ChatGPT the rest of my script, including the part where I suggested bind_rows()… glad it came up with that on its own!\nHere’s the refactored code it suggested:\nlibrary(dplyr)\n\n# Function to apply calc_lm across groups\nresults_df &lt;- penguins_filtered %&gt;%\n  split(list(.$species, .$sex), drop = TRUE) %&gt;%  # Split by species and sex\n  lapply(function(df) {\n    result &lt;- calc_lm(df, \"bill_length_mm\", \"bill_depth_mm\")  # Apply function\n    result$species &lt;- unique(df$species)  # Add species info\n    result$sex &lt;- unique(df$sex)          # Add sex info\n    return(result)\n  }) %&gt;%\n  bind_rows()  # Combine into a single dataframe\nThe results are functionally identical (though some unimportant differences in format - e.g., the order of rows, or the row names).\n\n\n\n\n\n\n\n\n\n\n2.4 Writing unit tests\nWhen writing functions, it is good practice to write unit tests to ensure that the function works as expected. A unit test is a bit of code that verifies the accuracy of a small, isolated block of application code (i.e., a small function). Typically they run a function with known inputs and test to make sure the outputs match expectations. ChatGPT can help you write unit tests for your functions, which can be especially helpful if you are not familiar with them.\n\n\n\n\n\n\nExercise 9\n\n\n\n\n\n\nLet’s ask ChatGPT to write a unit test for our calc_lm() function. This is a simple function that takes two vectors of data, runs a linear regression, and reports the slope, intercept, and p value.\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nHere is what ChatGPT suggested:\nlibrary(testthat)\n\n# Load the function (if not already sourced)\n# source(\"your_script.R\")\n\n# Create a test dataset\ntest_data &lt;- data.frame(\n  x = c(1, 2, 3, 4, 5),\n  y = c(2, 4, 6, 8, 10)  # Perfect linear relationship (y = 2x)\n)\n\ntest_that(\"calc_lm computes correct slope, intercept, and p-value\", {\n  result &lt;- calc_lm(test_data, \"y\", \"x\")\n  \n  # Expected values\n  expected_slope &lt;- 2  # Since y = 2x\n  expected_intercept &lt;- 0  # No intercept\n  expected_p_value &lt;- 2.2e-16  # Should be very close to zero due to perfect correlation\n  \n  # Check if slope and intercept are as expected\n  expect_equal(result$slope, expected_slope, tolerance = 1e-6)\n  expect_equal(result$intercept, expected_intercept, tolerance = 1e-6)\n  \n  # Check if p-value is very small (indicating strong statistical significance)\n  expect_lt(result$p_value, 0.05)\n})\nThis uses the testthat package, designed specifically for unit testing. It creates a test dataset with a perfect linear relationship, feeds it into the function, and compares the output values to our expectations. It is a good start, but when running it, there is an unexpected snag. The test doesn’t take into account that summary() on a perfectly linear relationship throws a warning, causing the unit test to fail. We could easily adjust the test to account for this, e.g., by adding a little noise into the test data to make it not perfectly linear, but the important thing is that we as the coder must verify that the code works as expected.\nJust to see what would happen, I told ChatGPT about the warning:\n\n\n\n\n\nAnd it adjusted the unit test to incorporate a small amount of noise, just as I would have done!\nlibrary(testthat)\n\n# Load the function (if not already sourced)\n# source(\"your_script.R\")\n\n# Create a test dataset with slight randomness to avoid perfect collinearity\nset.seed(123)  # For reproducibility\ntest_data &lt;- data.frame(\n  x = c(1, 2, 3, 4, 5),\n  y = c(2, 4, 6, 8, 10) + rnorm(5, mean = 0, sd = 0.1)  # Adding slight noise\n)\n\ntest_that(\"calc_lm computes correct slope, intercept, and p-value\", {\n  result &lt;- calc_lm(test_data, \"y\", \"x\")\n  \n  # Expected values (approximately)\n  expected_slope &lt;- 2\n  expected_intercept &lt;- 0\n  \n  # Check if slope and intercept are close to expected values\n  expect_equal(result$slope, expected_slope, tolerance = 0.1)\n  expect_equal(result$intercept, expected_intercept, tolerance = 0.1)\n  \n  # Check if p-value indicates strong statistical significance\n  expect_lt(result$p_value, 0.05)\n})\n\n\n\n\n\n\n\n\n\n\n2.5 Translating code from one language to another\nIf you are proficient in one language but not another, ChatGPT can help you translate code from one language to another. This can be especially helpful as you are learning a new language, or if some functionality available in an unfamiliar language is not available in your preferred language.\n\n\n\n\n\n\nExercise 10\n\n\n\n\n\n\nLet’s try giving ChatGPT the nested loop code again, but this time ask to translate to Python.\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nHere’s what it returned for me, nearly a line-by-line translation with a note: “Here’s the equivalent Python code using pandas and list comprehensions for efficiency.”\nHowever, note that the code is not stand-alone, since it calls the function calc_lm() that we defined previously but in R, and calls for the penguins_filtered object which has not yet been loaded and filter in Python. But those can be fixed pretty easily! (e.g., ask ChatGPT to rewrite calc_lm in Python, and suggest code for loading/filtering the penguins data in Python).\n#| eval: false\n\nimport pandas as pd\n\n# Create an empty list to store results\nresults = []\n\n# Iterate over species\nfor species in penguins_filtered['species'].unique():\n    # Iterate over sex\n    for sex in penguins_filtered['sex'].unique():\n        # Filter the DataFrame for the specific species and sex\n        subset = penguins_filtered[(penguins_filtered['species'] == species) & \n                                   (penguins_filtered['sex'] == sex)]\n        \n        # Apply the function to the filtered data\n        result = calc_lm(subset, \"bill_length_mm\", \"bill_depth_mm\")\n        \n        # Add species and sex information\n        result['species'] = species\n        result['sex'] = sex\n        \n        # Append the result to the list\n        results.append(result)\n\n# Combine the results into a single DataFrame\nresults_df = pd.concat(results, ignore_index=True)\nAfter the code, it listed some changes/improvements it made, as well, which is helpful.\n\n\n\n\nAnd as long as we’re here, let’s ask it to refactor this code in Python instead of R:\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nAnd again, it provided a little context, and some nice tidy code (with the same limitations noted above). Note this refactored version in Python looks pretty different from the refactored version in R, but shares some functional similarities.\n#| eval: false\nimport pandas as pd\n\n# Define a function to process each group\ndef process_group(group):\n    result = calc_lm(group, \"bill_length_mm\", \"bill_depth_mm\")  # Apply function\n    result[\"species\"] = group[\"species\"].iloc[0]  # Assign species\n    result[\"sex\"] = group[\"sex\"].iloc[0]  # Assign sex\n    return result\n\n# Apply function across groups and combine results\nresults_df = pd.concat(\n    [process_group(group) for _, group in penguins_filtered.groupby([\"species\", \"sex\"])],\n    ignore_index=True\n)\n\n\n\n\n\n\n\n\n\n\n2.6 Summary: ChatGPT for coding\n\n\n\n\n\n\nSummary: ChatGPT tips\n\n\n\n\nChatGPT can provide coding support by:\n\ngenerating code from pseudocode\ncommenting existing code\nsuggesting improvements to code (refactoring)\ntranslating code from one language to another\n\nAs with Copilot, when you provide more context to ChatGPT (as pseudocode, example code, desired packages, etc), you will get better results\nAs with Copilot, you are responsible for the final product. Test the code, check for errors, and make sure it works as expected!",
    "crumbs": [
      "AI for Coding Support"
    ]
  },
  {
    "objectID": "s01_ai_coding_support.html#conclusions",
    "href": "s01_ai_coding_support.html#conclusions",
    "title": "AI for Coding Support",
    "section": "3 Conclusions",
    "text": "3 Conclusions\nIn this lesson we’ve introduced you to two different tools for leveraging generative AI tools to boost your coding practice. We encourage you to try out both tools and approaches, and mix and match to see what works best for you.",
    "crumbs": [
      "AI for Coding Support"
    ]
  },
  {
    "objectID": "s01_ai_coding_support.html#further-considerations",
    "href": "s01_ai_coding_support.html#further-considerations",
    "title": "AI for Coding Support",
    "section": "4 Further considerations",
    "text": "4 Further considerations\nWhat about hallucinations and errors?\n\nLLMs and generative AI are known to “hallucinate” and make up incorrect information from time to time. In coding, this may be less of an issue, since you can run the code to test it and verify that the results match expectations.\n\nHowever, if you are asking an LLM to code something that you don’t fully understand, you may not recognize errors or incorrect results. It might be risky to ask an LLM to write code for you that you wouldn’t be able to write yourself if given the time.\nLLMs are essentially trying to provide the most likely completion to a given prompt, i.e., the least surprising answer. For creative writing tasks, this can result in pretty mediocre and predictable prose. But for a highly structured task like coding, surprise is not the goal! An LLM trained on a massive code base will have a pretty good idea of the most common - and least surprising - ways to approach a typical coding problem.\n\nWhat about ethics?\n\nLLMs and generative AI are trained on large datasets, often including content scraped from artists and writers (and coders) without their consent. This raises ethical concerns about the use of generative AI for creative tasks, for plagiarism and exploitation of writers’ work and for the potential for putting artists out of work. For coding tasks this may be less of an issue, as much of the codebase is open source, e.g., public repos on Github or Q&A on Stack Overflow, though even open source material is often licensed with rules for attribution that may be violated by AI-generated code.\nWhile an LLM doesn’t ingest the data or information you put into your prompts (once it is trained, it can’t learn new things), it is still important to consider security and privacy risks. Including sensitive or private information in a prompt may result in some of that sensitive information being exposed in the resulting code. If you are coding an analysis that requires privacy or security, the LLM may not properly understand those privacy or security concerns and could generate code with security liabilities.\n\nWill this be problematic for the future of coding?\n\nDefinitely! With such tools readily at hand, then students won’t learn how to properly code - they will just have to rely on LLMs to do everything for them. Additionally, if a senior analyst can use an LLM to write code, they won’t need to hire junior analysts, thus breaking the pipeline for entry level coding jobs.\nCertainly not! With such tools readily at hand, the barrier to entry for coding is lowered. Consider that we’ve progressed from punch cards to binary to assembly language to FORTRAN to COBOL to C to R to Python to RStudio to Jupyter notebooks to LLMs. Each step has made coding more human readable and comprehensible, and yet we still have demand for coders. Instead of coding in R or Python (or binary), we’ll soon be coding in English (or Spanish, Chinese, etc). Coding will no longer be a barrier to statistical analysis, so junior and senior analysts can focus on the science!",
    "crumbs": [
      "AI for Coding Support"
    ]
  },
  {
    "objectID": "s04_ai_lit_review.html",
    "href": "s04_ai_lit_review.html",
    "title": "AI-assisted Lit Review",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\ntbd",
    "crumbs": [
      "AI-assisted Lit Review"
    ]
  },
  {
    "objectID": "s03_ai_and_apis.html",
    "href": "s03_ai_and_apis.html",
    "title": "Accessing the ChatGPT API",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\ntbd",
    "crumbs": [
      "Accessing the ChatGPT API"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About the sessions",
    "section": "",
    "text": "The National Center for Ecological Analysis and Synthesis (NCEAS), a research affiliate of UCSB, is now celebrating its 30th year. As part of this celebration, NCEAS is focusing on a theme of “AI for the Planet,” and as members of the NCEAS Learning Hub we have designed this set of informal interactive workshop sessions in support of that theme. In these sessions, we will explore uses of generative artificial intelligence (genAI or simply AI) such as ChatGPT and GitHub Copilot to help support NCEAS residents and others in applying AI tools to research, analysis, and writing. We are by no means experts in the design and application of AI, we are just curious and excited (perhaps a bit apprehensive) about the possibilities these new tools offer. These sessions may eventually become formal modules available for various NCEAS Learning Hub courses, but for now we just want to encourage the NCEAS community to explore the benefits and pitfalls these tools offer.",
    "crumbs": [
      "About the sessions"
    ]
  },
  {
    "objectID": "index.html#ai-for-nceas",
    "href": "index.html#ai-for-nceas",
    "title": "About the sessions",
    "section": "",
    "text": "The National Center for Ecological Analysis and Synthesis (NCEAS), a research affiliate of UCSB, is now celebrating its 30th year. As part of this celebration, NCEAS is focusing on a theme of “AI for the Planet,” and as members of the NCEAS Learning Hub we have designed this set of informal interactive workshop sessions in support of that theme. In these sessions, we will explore uses of generative artificial intelligence (genAI or simply AI) such as ChatGPT and GitHub Copilot to help support NCEAS residents and others in applying AI tools to research, analysis, and writing. We are by no means experts in the design and application of AI, we are just curious and excited (perhaps a bit apprehensive) about the possibilities these new tools offer. These sessions may eventually become formal modules available for various NCEAS Learning Hub courses, but for now we just want to encourage the NCEAS community to explore the benefits and pitfalls these tools offer.",
    "crumbs": [
      "About the sessions"
    ]
  },
  {
    "objectID": "index.html#nceas-expertise",
    "href": "index.html#nceas-expertise",
    "title": "About the sessions",
    "section": "2 NCEAS Expertise",
    "text": "2 NCEAS Expertise\nNCEAS is a leading expert on interdisciplinary data science and works collaboratively to answer the world’s largest and most complex questions. The NCEAS approach leverages existing data and employs a team science philosophy to squeeze out all potential insights and solutions efficiently - this is called synthesis science.\nNCEAS has 30 years of success with this model among working groups and environmental professionals. In conjunction with the NCEAS Learning Hub, we are excited to pass along skills, workflows, and mindsets to help the NCEAS community better understand the implications of AI for the Planet.\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nPractice strategies for using AI tools effectively and responsibly in coding, analysis, and research.\nExplore the ethical implications of using AI in research and how to navigate them.\nFeel comfortable with using various AI tools in your own work.",
    "crumbs": [
      "About the sessions"
    ]
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "About the sessions",
    "section": "3 Code of Conduct",
    "text": "3 Code of Conduct\nBy participating in this activity you agree to abide by the NCEAS Code of Conduct.",
    "crumbs": [
      "About the sessions"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "About the sessions",
    "section": "4 About this book",
    "text": "4 About this book\nThese written materials are the result of a continuous and collaborative effort at NCEAS to help researchers make their work more transparent and reproducible. This work began in the early 2000’s, and reflects the expertise and diligence of many, many individuals. The primary authors are listed in the citation below, with additional contributors recognized for their role in developing previous iterations of these or similar materials.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCitation: Casey O’Hara and Li Kui (2025), AI for NCEAS. URL https://nceas-learning-hub.github.io/ai_for_nceas.\nAdditional contributors: tbd\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "About the sessions"
    ]
  }
]