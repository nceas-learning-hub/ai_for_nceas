{
  "hash": "8f4cb3eb20fbc0cbcd2eb48eed179671",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: AI for Coding Support\n\nexecute:\n  echo: false\n---\n\n\n\n\n:::{.callout-tip}\n## Learning objectives\n\nBy the end of this session, you should be able to:\n\n* Effectively use Github Copilot in RStudio:\n    * Configure RStudio to work with GitHub Copilot\n    * Use comments and pseudocode to guide Copilot to assist with coding tasks\n* Leverage ChatGPT (or another Large Language Model) to assist with coding tasks:\n    * Translate pseudocode into functional code\n    * Comment existing code\n    * Suggest improvements to code for performance or readability\n    * Write unit tests for functions\n    * Translate code from one language to another (e.g., R vs. Python)\n:::\n\n\n![](images/ai_coding_support/english.png){width=60% fig-alt=\"The hottest new programming language is English - Andrej Karpathy\"}\n\nWe can use GitHub Copilot and/or Large Language Models like ChatGPT or Claude to help us write code in R, Python, or other languages, using natural language such as English.  It might be handy to consider these as two different avenues of leveraging AI in coding:\n\n* Active coding support with GitHub Copilot: Copilot is like an assistant sitting next to you at your computer, writing code along with you, in real time - i.e., pair programming.\n* Iterative coding support with ChatGPT: ChatGPT is more like an assistant in the next office over, not writing code in real time with you, but collaborating through back-and-forth interactions and prompts.\n\n## Active coding support with GitHub Copilot\n\nGitHub Copilot is an AI-powered code completion tool that helps you write code faster and with fewer errors. It is effectively \"pair programming\" where you and a partner (in this case, an AI partner) code together in real time.\nIt is available as a plugin for IDEs such as Visual Studio Code and RStudio.  Copilot is a large language model (LLM) related to ChatGPT, trained on GitHub and other codebases, and it can generate code completions in real time based on the context of your coding task.  In this lesson, we will configure RStudio to work with GitHub Copilot and practice techniques to use Copilot effectively to assist with coding tasks.\n\nIf you have an [education GitHub account](https://github.com/education) (e.g., an .edu email address that is associated with your GitHub account), you should be able to get free access to Copilot Pro.  Otherwise you can access a free but limited version of Copilot, which should be adequate for this session.  See [more Copilot requirements here](https://docs.posit.co/ide/user/ide/guide/tools/copilot.html).\n\n::: {.callout-note}\n\nHere we will focus on using AI code completion tools in R and RStudio, since it is a popular language and IDE for ecologists, but the same methods work for other languages and IDEs.  VS Code is a Microsoft product, as is Copilot (and GitHub for that matter), so its integration is even better than RStudio.\n\n:::\n\n### Configuring RStudio to work with GitHub Copilot\n\n[Full Screen](slides/ai_coding_support/slides.html)\n\n\n\n\n```{=html}\n<iframe class=\"slide-deck\" src=\"slides/ai_coding_support/slides.html\" height=\"337px\" width=\"600px\" style=\"border: 1px solid #2e3846;\"></iframe>\n```\n\n\n\n\n### Using Copilot to assist with coding tasks\n\nYou need a fundamental understanding of coding to be able to use Copilot effectively.  It can make intelligent guesses as to your intentions, but doesn't necessarily know where your files are, or what you intend to do with the data.  So it is helpful to guide Copilot in the right direction using comments, sentences, and \"pseudocode\" before you start any actual coding.  The more information you provide through your Markdown text and code-chunk comments, the better Copilot will be able to help you.\n\n::: {.callout-tip}\n## Pseudocode\n\nPseudocode is a way of planning out an analysis by writing out the sequence of steps you expect to take, in plain language, perhaps as bullet points.  There are more formalized versions of pseudocode, and less formalized (see [\"vibecoding\"](https://www.oneusefulthing.org/p/speaking-things-into-existence?utm_source=post-email-title&publication_id=1180644&post_id=148596183)) but here we take a middle road, and we will stick with just plain English sentences to communicate our intentions.  Some tips:\n\n* Specify whether you want to use the `tidyverse` metapackage or base R functions (or another language, e.g., Python!).\n* If you know the names of certain functions, it is helpful to use those in your pseudocode, to help Copilot incorporate those into its code completions.\n* Use comments to signal to Copilot what you'd like to do at each step.\n\n:::\n\n::: {.callout-exercise}\n\nCreate a code chunk, read in some data, do some data wrangling, and then make a plot with the resulting data.  Use comments in your code chunk to signal to Copilot what you'd like to do at each step.  Here are the steps we will do, using a `tidyverse` workflow:\n\n* load the `palmerpenguins` package and load the `penguins` data\n* drop any incomplete observations (any NA values)\n* filter to just keep the Chinstrap and Adelie species\n* generate a scatter plot of bill length vs. bill depth for each species\n* then, for the two species, run a t-test to determine whether the bill lengths are statistically different between the two species.  Print a summary of the t test results.\n\n_(Copy the above text and paste into a Quarto document, or rephrase it in your own words, then create a code chunk and let Copilot suggest some code!)_\n\n::::{.callout-answer}\n\nHere's what it looked like when I let Copilot help me write the code - I accepted each line by pressing \"tab\".  This might change entirely if I ran the exercise again.  \n\n* How different does it look from the code you wrote?\n* How does it differ from how you might code it on your own?\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](s01_ai_coding_support_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  bill_length_mm by species\nt = -21.712, df = 108.17, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group Adelie and group Chinstrap is not equal to 0\n95 percent confidence interval:\n -10.92365  -9.09605\nsample estimates:\n   mean in group Adelie mean in group Chinstrap \n               38.82397                48.83382 \n```\n\n\n:::\n\n::: {.cell-output-display}\n\n\n|  estimate| estimate1| estimate2| statistic| p.value| parameter|  conf.low| conf.high|method                  |alternative |\n|---------:|---------:|---------:|---------:|-------:|---------:|---------:|---------:|:-----------------------|:-----------|\n| -10.00985|  38.82397|  48.83382|  -21.7125|       0|  108.1722| -10.92365|  -9.09605|Welch Two Sample t-test |two.sided   |\n\n\n:::\n:::\n\n\n\n\n::::\n:::\n\n::: {.callout-exercise}\nLet's try the same exercise but this time let's only use base R functions:\n\n* load the `palmerpenguins` package and load the `penguins` data\n* drop incomplete observations\n* filter to just keep the Chinstrap and Adelie species\n* generate a scatter plot of bill length vs. bill depth for each species\n* then, for the two species, run a t-test to determine whether the bill lengths are statistically different between the two species.  Print a summary of the t test results.\n\n_(Below the previous code chunk, in a Markdown section, copy and paste the above text, or simply write a sentence along the lines of \"Repeat the previous analysis using only base R functions and plotting.\"  Then create a new code chunk and let Copilot suggest code for you.)_\n\n:::: {.callout-answer}\n\nHere is what Copilot came up with for me.  Note, it knew from above that I wanted to use base R functions this time instead of `tidyverse` functions!\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](s01_ai_coding_support_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Length Class  Mode     \nstatistic   1      -none- numeric  \nparameter   1      -none- numeric  \np.value     1      -none- numeric  \nconf.int    2      -none- numeric  \nestimate    2      -none- numeric  \nnull.value  1      -none- numeric  \nstderr      1      -none- numeric  \nalternative 1      -none- character\nmethod      1      -none- character\ndata.name   1      -none- character\n```\n\n\n:::\n:::\n\n\n\n\n::::\n:::\n\n::: {.callout-warning title=\"Copilot limitations!\"}\n\nCopilot cannot read your mind, and it may make incorrect assumptions about what you want to do with your data and analysis.  It is important to understand the code that Copilot is generating for you, and to check it for errors!  \n\n* Copilot can only _add_ code or text; it cannot subtract or edit buggy or incorrect code or text without specific prompting from the user.\n* You can always reject Copilot's suggestions and write your own code, or accept its suggestions as a starting point then edit them as you see fit before moving on.\n* You get the last word (no matter how hard Copilot tries!), and you as the coder are responsible for the final product.\n\n:::\n\n### Using Copilot for more complex coding tasks\n\nCopilot can also help you write functions and loops, among other things.\n\n#### Copilot to write a function\n\n::: {.callout-exercise}\nLet's write a function that takes a dataframe of data and for two given columns, calculates the slope, intercept, and p value for a linear regression model.  Let's call it `calc_lm()`.  Some pseudocode to get you started:\n\n* identify and pull out the y (outcome) and x (predictor) variables as columns in the data frame\n* perform a linear regression on `y ~ x`\n* return the slope, intercept, and p value in a dataframe.\n\n_(Feel free to copy the above text and paste into a Markdown section of your Quarto document, then let Copilot make its suggestions)_\n\n:::: {.callout-answer}\n\nHere's what Copilot suggested for me, line by line, as I was writing this lesson.  After finishing the function, I started writing a comment to test, it assumed I wanted to use data that I had mentioned earlier in the script, i.e., `penguins_filtered`.\n\n```r\n# function to calculate slope, intercept, and p value for a linear regression model\ncalc_lm <- function(data, y, x) {\n  # identify and pull out the y (outcome) and x (predictor) variables as columns in the data frame\n  y <- data[[y]]\n  x <- data[[x]]\n  \n  # perform a linear regression on y ~ x\n  lm_model <- lm(y ~ x, data = data)\n  \n  # return the slope, intercept, and p value in a dataframe\n  data.frame(slope = coef(lm_model)[2],\n             intercept = coef(lm_model)[1],\n             p_value = summary(lm_model)$coefficients[2, 4])\n}\n### test on the penguins data\ncalc_lm(penguins_filtered, \"bill_length_mm\", \"bill_depth_mm\")\n```\n\nDoes Copilot's suggestion work for you?  If not take a moment to fix it and make it work.  For me, it created a perfectly good function the first time.  Some observations on the code presented here:\n\n* I would have avoided overwriting the local value of `x` and `y` in the function, as it could be confusing.\n* Note that the p value it selects from the linear model is that of the slope, not the intercept.  That is probably what we wanted, but would be a good idea to make sure that meets our expectations!\n* I might also prefer to have the output in a different format, perhaps using `broom::tidy()`.\n\n::::\n:::\n\n#### Copilot to write a loop\n\n::: {.callout-exercise}\nNow let's get Copilot to help use the previous function to iterate over the various species by sex.  Using for-loops, this will involve nested loops (one for species, one for sex), so let's see if Copilot can figure that out!  Pseudocode:\n\n* For each species, and each sex, apply the function above to the penguins data, and add the species and sex to the resulting dataframe.\n* Store the results in a list of dataframes\n* Then combine the results into a single dataframe.\n\n_(Again, copy the text above and paste into a Markdown section, open a new code chunk and let Copilot make suggestions.  It may struggle with this, so if it gets stuck, help it by writing some code or comments_)\n\n:::: {.callout-warning}\nI almost certainly wouldn't have actually used a `for` loop here in the first place - better options would be a `group_by() %>% summarize()` workflow or `purrr::map2` - but we're just trying to see what Copilot can do with a more complex coding task!\n::::\n\n:::: {.callout-answer}\n\nHere's what Copilot suggested, based on the pseudocode and previous code chunks.  However, this was a little more complex and required some guidance, hand holding, and bug-fixing.\n\n```r\n# create an empty list to store the results\nresults <- list()                                                 # <1>\ni <- 1                                                            # <2>\n\n# iterate over the species\nfor (species in unique(penguins_filtered$species)) {\n  # iterate over the sex\n  for(sex in unique(penguins_filtered$sex)) {                     # <3>\n    # apply the function above to the penguins data\n    result <- calc_lm(penguins_filtered[penguins_filtered$species == species & penguins_filtered$sex == sex, ], \"bill_length_mm\", \"bill_depth_mm\")                                                  # <4>\n    # add the species and sex to the resulting dataframe\n    result$species <- species\n    result$sex <- sex                                             # <5>\n    # store the results in the list\n    results[[i]] <- result                                        # <6>\n    i <- i + 1                                                    # <7>\n  }\n}\n\n# combine the results into a single dataframe\nresults_df <- do.call(rbind, results)\n  \n```\n1. Copilot didn't pre-allocate space for the list based on the number of expected entries.  Not a dealbreaker for a small project, but generally a good idea.\n2. After Copilot started the loop, it didn't include an indexing variable, and hadn't considered that the \"species\" index would be character, not numeric, so would be challenging to use as an indexing variable after the fact.  I manually added in an index `i` to keep track of the list index.\n3. Copilot struggled with this nested loop format - so I had to manually start typing this line; but then Copilot was able to finish.\n4. Again, Copilot struggled here - it wrote the function call halfway through the filtered step; when I wrote the `penguins_filtered$sex == sex` part, it was able to finish the line.  However, it made some errors finishing the line as well!\n5. Once again, Copilot wasn't sure how to add the sex info to the resulting dataframe, so I had to manually start writing.\n6. Here, Copilot used a single bracket for indexing, which is incorrect for a list in R.  I had to manually change this to double brackets.\n7. Copilot didn't include an incrementing index variable, so I had to manually add this.\n\nIn summary, Copilot was helpful in getting the ball rolling here, but needed lots of manual help.  Additionally, it made some choices I probably would have made differently - e.g., I probably would have filtered the `penguins_filtered` dataframe *before* the `calc_lm` call, and probably would have used `dplyr::mutate` to add the species and sex columns to the result.  Finally, I probably would have used `bind_rows()` instead of `do.call(rbind, results)` to combine the results, but that's just what I'm used to.\n \n::::\n:::\n\n### Copilot for coding: summary\n\nCopilot can be very helpful at speeding up your coding, especially for simple or well-defined tasks.  One potential pitfall is that using Copilot in this way has a certain amount of path dependency.  Once a bit of code is written in the script, Copilot takes that as given - it can't modify or erase existing code, it can only keep going forward by adding.\n\nIn the next section, we'll look at how to use ChatGPT to help with coding in a more interactive/iterative fashion, which can avoid path dependency but may introduce other complexities.\n\n::: {.callout-tip title=\"Summary: Copilot tips\"}\n\n* Before starting your analysis, prime Copilot by writing out an overview of your analysis, including packages, functions, and statistical tests you want to do.\n* While writing code, if Copilot gets stuck, or treads down an unproductive path, you have several options:\n    * write out a comment or additional pseudocode to guide Copilot in the right direction.\n    * accept Copilot's suggestion (if it's on the right track) and manually fix it before continuing.\n    * reject Copilot's suggestion (if it's way off base) and write your own code, to signal what you really want\n* Run the code to make sure it works as expected, and check for errors.  You are responsible for the final result.\n* Remember, you can turn Copilot off entirely, or set it to autocomplete only when requested:\n    * Tools --> Global Options --> Copilot --> Show Copilot code suggestions: Manually (Ctrl-Backslash)).\n    \n:::\n\n## Iterative coding support with ChatGPT\n\nIn this section we'll look at using ChatGPT outside of RStudio.  Here we can take existing code and feed it into ChatGPT and ask it to help us with various tasks, or feed pseudocode into ChatGPT and ask it to generate code for us.  Rather than line by line, like Copilot, an LLM can often provide a more complete solution that can then be copied/pasted back into RStudio.  It can also iterate through multiple rounds of conversation/prompts.\n\n### Pseudocode into code\n\nJust as Copilot can generate code from pseudocode, ChatGPT can do the same.  You can then copy/paste the results into RStudio.\n\n::: {.callout-exercise} \n\nLet's give ChatGPT the same sequence of steps we used above.  We will copy and paste the pseudocode from above (repeated here) into ChatGPT and ask it to generate code for us.\n \n* load the `palmerpenguins` package and load the `penguins` data\n* drop any incomplete observations (any NA values)\n* filter to just keep the Chinstrap and Adelie species\n* generate a scatter plot of bill length vs. bill depth for each species\n* then, for the two species, run a t-test to determine whether the bill lengths are statistically different between the two species.  Print a summary of the t test results.\n\n![](images/ai_coding_support/chatgpt_pseudocode1.png){width=600 fig-align=\"center\"}\n\n:::: {.callout-answer}\n\nHere is the code it generated for me.  Note it chose a (mostly) `tidyverse` approach this time, though I didn't explicitly prompt it to do so: \n\n```r\n# Load necessary packages\nlibrary(palmerpenguins)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Load the penguins dataset and remove incomplete observations\npenguins_clean <- penguins %>%\n  na.omit() %>%\n  filter(species %in% c(\"Chinstrap\", \"Adelie\"))\n\n# Generate scatter plot of bill length vs. bill depth\nggplot(penguins_clean, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point() +\n  labs(title = \"Bill Length vs. Bill Depth\",\n       x = \"Bill Length (mm)\",\n       y = \"Bill Depth (mm)\") +\n  theme_minimal()\n\n# Perform a t-test on bill length between Chinstrap and Adelie species\nt_test_result <- t.test(bill_length_mm ~ species, data = penguins_clean)\n\n# Print summary of t-test results\nprint(t_test_result)\n\n```\n\nThis code works exactly as desired, though we set it to not evaluate here in the script, for sake of space.\n\n::::\n:::\n\n::: {.callout-exercise}\nI could follow up with a request to not use any `tidyverse` packages...\n\n![](images/ai_coding_support/chatgpt_pseudocode2.png){width=600 fig-align=\"center\"}\n\n:::: {.callout-answer}\nAnd this is what it came up with:\n\n```r\n# Load the palmerpenguins package\nlibrary(palmerpenguins)\n\n# Load the dataset and remove incomplete observations\npenguins_clean <- na.omit(penguins)\n\n# Keep only Chinstrap and Adelie species\npenguins_clean <- penguins_clean[penguins_clean$species %in% c(\"Chinstrap\", \"Adelie\"), ]\n\n# Generate scatter plot of flipper length vs. bill depth\nplot(penguins_clean$bill_length_mm, penguins_clean$bill_depth_mm,\n     col = ifelse(penguins_clean$species == \"Chinstrap\", \"blue\", \"red\"),\n     pch = 19,\n     xlab = \"Bill Length (mm)\",\n     ylab = \"Bill Depth (mm)\",\n     main = \"Bill Length vs. Bill Depth\")\nlegend(\"topright\", legend = c(\"Chinstrap\", \"Adelie\"), col = c(\"blue\", \"red\"), pch = 19)\n\n# Perform a t-test on flipper length between Chinstrap and Adelie species\nt_test_result <- t.test(bill_length_mm ~ species, data = penguins_clean)\n\n# Print summary of t-test results\nprint(t_test_result)\n\n```\n\nThis code works exactly as desired, though we set it to not evaluate here in the script, for sake of space.\n\n::::\n:::\n\n![](images/ai_coding_support/thank_you.jpg){.lightbox width=300 fig-align=\"center\"}\n\n### Commenting existing code\n\nOften when we are trying to code, we Google to find a solution online (e.g., Stack Overflow), but the code is not familiar to us or difficult to understand.  ChatGPT can take that code and insert helpful comments to explain what each line is doing.  \n\n::: {.callout-exercise}\n\nHere is a complex Python script that would be challenging to understand for someone not proficient in Python:\n\nhttps://raw.githubusercontent.com/convei-wwf/NLParty/refs/heads/main/abstract_classifier.py\n\n![](images/ai_coding_support/chatgpt_comment.png){width=600 fig-align=\"center\"}\n\n:::: {.callout-answer}\nHere's what it came up with! Try asking ChatGPT to comment for you as if you were completely new to Python and see how it changes.\n\n```python\n#| eval: false\n\nimport argparse  # For parsing command-line arguments\nimport glob  # For file pattern matching\nimport logging  # For logging messages\nimport os  # For operating system interactions\nimport time  # For measuring execution time\nimport re  # For regular expressions\nimport warnings  # For handling warnings\nfrom transformers import pipeline  # For using Hugging Face's transformers pipeline\nimport torch  # For PyTorch operations\n\n# Suppress specific UserWarnings from torch.utils.data\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.utils.data\")\n\n# Configure logging settings\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format=(\n        '%(asctime)s (%(relativeCreated)d) %(levelname)s %(name)s'\n        ' [%(funcName)s:%(lineno)d] %(message)s'\n    )\n)\nlogging.getLogger('taskgraph').setLevel(logging.INFO)\nLOGGER = logging.getLogger(__name__)\n\ndef main():\n    # Check if CUDA (GPU support) is available and print the result\n    print(torch.cuda.is_available())\n\n    # Set up argument parser for command-line inputs\n    parser = argparse.ArgumentParser(description='Affiliation classifier')\n    parser.add_argument('bib_file_pattern', help='path to bibliography list')\n    parser.add_argument('abstract_tag_file', help='path to abstract tags')\n    parser.add_argument('--target_path', help='target classified table')\n    args = parser.parse_args()\n\n    # Load bibliography files matching the given pattern\n    print('load bib_file')\n    affiliation_set = set()\n    article_count = 0\n    for bib_file in glob.glob(args.bib_file_pattern):\n        with open(bib_file, 'r', encoding='utf-8') as file:\n            affiliation_str = None\n            abstract_str = None\n            article_id = None\n            for line in file:\n                try:\n                    # Extract article ID using regex\n                    article_id = re.search('@[^{]+{(.*),', line).group(1)\n                    if article_id is None:\n                        print(f'ERROR: {line}')\n                        affiliation_str = None\n                        abstract_str = None\n                        continue\n                except:\n                    pass\n                # Extract abstract content\n                if 'abstract =' in line:\n                    abstract_str = re.search('{(.*)}', line).group(1)\n                # Extract affiliations content\n                elif 'affiliations =' in line:\n                    affiliation_str = re.search('{(.*)}', line).group(1)\n                # If both abstract and affiliation are found, add to the set\n                if abstract_str and affiliation_str:\n                    if article_id is None:\n                        print(f'ERROR: {abstract_str}')\n                    affiliation_set.add((article_id, affiliation_str, abstract_str))\n                    article_count += 1\n                    # Reset variables for the next article\n                    article_id = None\n                    affiliation_str = None\n                    abstract_str = None\n\n    # Load candidate labels from the abstract tag file\n    print('load candidate_labels')\n    print(f'article count: {article_count} vs {len(affiliation_set)}')\n    with open(args.abstract_tag_file, 'r') as file:\n        candidate_labels = ', '.join([v for v in file.read().split('\\n') if len(v) > 0])\n    print(candidate_labels)\n\n    # Initialize the zero-shot classification pipeline\n    batch_size = 10\n    classifier = pipeline(\n        \"zero-shot-classification\",\n        model=\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\",\n        device=0,  # Use GPU if available\n        batch_size=batch_size,\n        truncation=True\n    )\n    print(classifier.model.device)\n\n    # Determine the target path for the output\n    target_path = args.target_path\n    if target_path is None:\n        target_path = '%s_classified%s' % os.path.splitext(args.affiliation_pickle_list)\n\n    total_time = 0\n    events = 0\n\n    # Open the target file for writing the classification results\n    with open(target_path, 'w', encoding='utf-8') as file:\n        print(f'opening {target_path} for writing {len(affiliation_set)} affiliations')\n        start_time = time.time()\n\n        # Generator function to yield affiliation strings\n        def affiliation_generator():\n            for _, affiliation_str, _ in affiliation_set:\n                yield affiliation_str\n\n        index = 1\n        # Perform classification and write results\n        for (article_id, affiliation_str, abstract_str), result in zip(\n            affiliation_set,\n            classifier(affiliation_generator(), candidate_labels, multi_label=True)\n        ):\n            file.write(f'{article_id}\\n{affiliation_str}\\n{abstract_str}\\n')\n            for label, score in zip(result['labels'], result['scores']):\n                file.write(f'{label}: {score}\\n')\n            file.write('\\n')\n            file.flush()\n            current_time = (time.time() - start_time)\n            events += 1\n            total_time += current_time\n            print(f'({index}/{len(affiliation_set)}) took {current_time}s to tag {article_id} '\n                  f'(time left: {total_time/events*(len(affiliation_set)-index)})')\n            start_time = time.time()\n            index += 1\n\nif __name__ == '__main__':\n    main()\n\n```\n \nNot only does ChatGPT comment the code for me, it also gives an overview of the script at the end.  I did not notice any changes to the code, except for the removal of some lines of code that had been commented out in the original (and were thus not functional).  But it would be important to verify functionality of the updated code!\n\n::::\n:::\n \n### Suggesting improvements to code (refactoring)\n\nOften we have a piece of code we've written, or borrowed from somewhere else, but it is slow or hard to read, and we want to improve it (\"refactoring\").  Perhaps we want to take a repeated process and streamline it or parallelize it across multiple processors.  ChatGPT can help with this as well.\n\n::: {.callout-tip}\n### Start a new chat or continue with existing chat?\n\nSince we've already chatted with the LLM, if we continue in the same conversation it will \"remember\" the previous context.  But perhaps we want to start fresh on a new problem.  \n\n* For example, if I asked ChatGPT to comment the previous code as if I were a novice in Python, it may continue to assume I'm a novice for future questions, which may produce overly simplified or verbose responses.  This might call for a new chat.\n* Alternately, if I tried a bit of suggested code and encountered an error, I can paste that error back into the same conversation and ask ChatGPT to alter its suggestion accordingly.  This would not be effective if I started a new chat.\n\n:::\n\n::: {.callout-exercise}\nLet's ask ChatGPT to take the nested for-loop above (a functional but not particularly elegant piece of code) and ask ChatGPT to recommend improvements.\n\n![](images/ai_coding_support/chatgpt_refactor.png){width=600 fig-align=\"center\"}\n\n:::: {.callout-answer}\nBefore offering refactored code, it offered some tips, quoted here:\n\n> Your code is functional, but it can be improved for efficiency and elegance. Here are some suggested improvements:\n> \n> Key Enhancements:\n> \n> * Avoid explicit indexing (`i <- 1`): Instead of manually managing an index for the list, you can use `append()` or `lapply()`.\n> * Use `split()` + `lapply()`: This avoids explicit for loops and makes the code more concise and efficient.\n> * Use `bind_rows()` from `dplyr`: Instead of `do.call(rbind, results)`, `dplyr::bind_rows()` is faster and more readable.\n\nNote that I didn't show ChatGPT the rest of my script, including the part where I suggested `bind_rows()`... glad it came up with that on its own!\n\nHere's the refactored code it suggested:\n\n```r\nlibrary(dplyr)\n\n# Function to apply calc_lm across groups\nresults_df <- penguins_filtered %>%\n  split(list(.$species, .$sex), drop = TRUE) %>%  # Split by species and sex\n  lapply(function(df) {\n    result <- calc_lm(df, \"bill_length_mm\", \"bill_depth_mm\")  # Apply function\n    result$species <- unique(df$species)  # Add species info\n    result$sex <- unique(df$sex)          # Add sex info\n    return(result)\n  }) %>%\n  bind_rows()  # Combine into a single dataframe\n\n```\n\nThe results are functionally identical (though some unimportant differences in format - e.g., the order of rows, or the row names).\n\n::::\n:::\n\n### Writing unit tests\n\nWhen writing functions, it is good practice to write unit tests to ensure that the function works as expected.  A unit test is a bit of code that verifies the accuracy of a small, isolated block of application code (i.e., a small function).  Typically they run a function with known inputs and test to make sure the outputs match expectations.  ChatGPT can help you write unit tests for your functions, which can be especially helpful if you are not familiar with them.\n\n::: {.callout-exercise}\nLet's ask ChatGPT to write a unit test for our `calc_lm()` function.  This is a simple function that takes two vectors of data, runs a linear regression, and reports the slope, intercept, and p value.  \n\n![](images/ai_coding_support/chatgpt_unittest1.png){width=600 fig-align=\"center\"}\n\n:::: {.callout-answer}\nHere is what ChatGPT suggested:\n\n```r\nlibrary(testthat)\n\n# Load the function (if not already sourced)\n# source(\"your_script.R\")\n\n# Create a test dataset\ntest_data <- data.frame(\n  x = c(1, 2, 3, 4, 5),\n  y = c(2, 4, 6, 8, 10)  # Perfect linear relationship (y = 2x)\n)\n\ntest_that(\"calc_lm computes correct slope, intercept, and p-value\", {\n  result <- calc_lm(test_data, \"y\", \"x\")\n  \n  # Expected values\n  expected_slope <- 2  # Since y = 2x\n  expected_intercept <- 0  # No intercept\n  expected_p_value <- 2.2e-16  # Should be very close to zero due to perfect correlation\n  \n  # Check if slope and intercept are as expected\n  expect_equal(result$slope, expected_slope, tolerance = 1e-6)\n  expect_equal(result$intercept, expected_intercept, tolerance = 1e-6)\n  \n  # Check if p-value is very small (indicating strong statistical significance)\n  expect_lt(result$p_value, 0.05)\n})\n```\n\nThis uses the `testthat` package, designed specifically for unit testing.  It creates a test dataset with a perfect linear relationship, feeds it into the function, and compares the output values to our expectations.  It is a good start, but when running it, there is an unexpected snag.  The test doesn't take into account that `summary()` on a perfectly linear relationship throws a warning, causing the unit test to fail.  We could easily adjust the test to account for this, e.g., by adding a little noise into the test data to make it not perfectly linear, but the important thing is that we as the coder must verify that the code works as expected.  \n\nJust to see what would happen, I told ChatGPT about the warning:\n\n![](images/ai_coding_support/chatgpt_unittest2.png){width=600 fig-align=\"center\"}\n\nAnd it adjusted the unit test to incorporate a small amount of noise, just as I would have done!\n\n```r\nlibrary(testthat)\n\n# Load the function (if not already sourced)\n# source(\"your_script.R\")\n\n# Create a test dataset with slight randomness to avoid perfect collinearity\nset.seed(123)  # For reproducibility\ntest_data <- data.frame(\n  x = c(1, 2, 3, 4, 5),\n  y = c(2, 4, 6, 8, 10) + rnorm(5, mean = 0, sd = 0.1)  # Adding slight noise\n)\n\ntest_that(\"calc_lm computes correct slope, intercept, and p-value\", {\n  result <- calc_lm(test_data, \"y\", \"x\")\n  \n  # Expected values (approximately)\n  expected_slope <- 2\n  expected_intercept <- 0\n  \n  # Check if slope and intercept are close to expected values\n  expect_equal(result$slope, expected_slope, tolerance = 0.1)\n  expect_equal(result$intercept, expected_intercept, tolerance = 0.1)\n  \n  # Check if p-value indicates strong statistical significance\n  expect_lt(result$p_value, 0.05)\n})\n\n```\n\n\n::::\n:::\n\n### Translating code from one language to another\n\nIf you are proficient in one language but not another, ChatGPT can help you translate code from one language to another.  This can be especially helpful as you are learning a new language, or if some functionality available in an unfamiliar language is not available in your preferred language.\n\n::: {.callout-exercise}\n\nLet's try giving ChatGPT the nested loop code again, but this time ask to translate to Python.\n\n![](images/ai_coding_support/chatgpt_translate.png){width=600 fig-align=\"center\"}\n\n:::: {.callout-answer}\n\nHere's what it returned for me, nearly a line-by-line translation with a note: \"Here's the equivalent Python code using **pandas** and **list comprehensions** for efficiency.\"\n\nHowever, note that the code is not stand-alone, since it calls the function `calc_lm()` that we defined previously but in R, and calls for the `penguins_filtered` object which has not yet been loaded and filter in Python.  But those can be fixed pretty easily!  (e.g., ask ChatGPT to rewrite `calc_lm` in Python, and suggest code for loading/filtering the penguins data in Python).\n\n```python\n#| eval: false\n\nimport pandas as pd\n\n# Create an empty list to store results\nresults = []\n\n# Iterate over species\nfor species in penguins_filtered['species'].unique():\n    # Iterate over sex\n    for sex in penguins_filtered['sex'].unique():\n        # Filter the DataFrame for the specific species and sex\n        subset = penguins_filtered[(penguins_filtered['species'] == species) & \n                                   (penguins_filtered['sex'] == sex)]\n        \n        # Apply the function to the filtered data\n        result = calc_lm(subset, \"bill_length_mm\", \"bill_depth_mm\")\n        \n        # Add species and sex information\n        result['species'] = species\n        result['sex'] = sex\n        \n        # Append the result to the list\n        results.append(result)\n\n# Combine the results into a single DataFrame\nresults_df = pd.concat(results, ignore_index=True)\n\n```\n\nAfter the code, it listed some changes/improvements it made, as well, which is helpful.\n\n::::\n\nAnd as long as we're here, let's ask it to refactor this code in Python instead of R:\n\n![](images/ai_coding_support/chatgpt_translate_refactor.png){width=600 fig-align=\"center\"}\n\n:::: {.callout-answer}\n\nAnd again, it provided a little context, and some nice tidy code (with the same limitations noted above).  Note this refactored version in Python looks pretty different from the refactored version in R, but shares some functional similarities.\n\n```python\n#| eval: false\nimport pandas as pd\n\n# Define a function to process each group\ndef process_group(group):\n    result = calc_lm(group, \"bill_length_mm\", \"bill_depth_mm\")  # Apply function\n    result[\"species\"] = group[\"species\"].iloc[0]  # Assign species\n    result[\"sex\"] = group[\"sex\"].iloc[0]  # Assign sex\n    return result\n\n# Apply function across groups and combine results\nresults_df = pd.concat(\n    [process_group(group) for _, group in penguins_filtered.groupby([\"species\", \"sex\"])],\n    ignore_index=True\n)\n\n```\n\n::::\n:::\n\n### Summary: ChatGPT for coding\n\n::: {.callout-tip title=\"Summary: ChatGPT tips\"}\n\n* ChatGPT can provide coding support by:\n    * generating code from pseudocode\n    * commenting existing code\n    * suggesting improvements to code (refactoring)\n    * translating code from one language to another\n* As with Copilot, when you provide more context to ChatGPT (as pseudocode, example code, desired packages, etc), you will get better results\n* As with Copilot, you are responsible for the final product.  Test the code, check for errors, and make sure it works as expected!\n\n:::\n\n## Conclusions\n\nIn this lesson we've introduced you to two different tools for leveraging generative AI tools to boost your coding practice.  We encourage you to try out both tools and approaches, and mix and match to see what works best for you.\n\n## Further considerations\n\nWhat about hallucinations and errors?  \n\n* LLMs and generative AI are known to \"hallucinate\" and make up incorrect information from time to time.  In coding, this may be less of an issue, since you can run the code to test it and verify that the results match expectations.  \n* However, if you are asking an LLM to code something that you don't fully understand, you may not recognize errors or incorrect results.  It might be risky to ask an LLM to write code for you that you wouldn't be able to write yourself if given the time.\n* LLMs are essentially trying to provide the most likely completion to a given prompt, i.e., the least surprising answer.  For creative writing tasks, this can result in pretty mediocre and predictable prose.  But for a highly structured task like coding, surprise is not the goal!  An LLM trained on a massive code base will have a pretty good idea of the most common - and least surprising - ways to approach a typical coding problem.\n\nWhat about ethics?  \n\n* LLMs and generative AI are trained on large datasets, often including content scraped from artists and writers (and coders) without their consent.  This raises ethical concerns about the use of generative AI for creative tasks, for plagiarism and exploitation of writers' work and for the potential for putting artists out of work.  For coding tasks this may be less of an issue, as much of the codebase is open source, e.g., public repos on Github or Q&A on Stack Overflow, though even open source material is often licensed with rules for attribution that may be violated by AI-generated code.\n* While an LLM doesn't ingest the data or information you put into your prompts (once it is trained, it can't learn new things), it is still important to consider security and privacy risks.  Including sensitive or private information in a prompt may result in some of that sensitive information being exposed in the resulting code.  If you are coding an analysis that requires privacy or security, the LLM may not properly understand those privacy or security concerns and could generate code with security liabilities.\n\nWill this be problematic for the future of coding?\n\n* Definitely!  With such tools readily at hand, then students won't learn how to properly code - they will just have to rely on LLMs to do everything for them.  Additionally, if a senior analyst can use an LLM to write code, they won't need to hire junior analysts, thus breaking the pipeline for entry level coding jobs.\n* Certainly not!  With such tools readily at hand, the barrier to entry for coding is lowered.  Consider that we've progressed from punch cards to binary to assembly language to FORTRAN to COBOL to C to R to Python to RStudio to Jupyter notebooks to LLMs.  Each step has made coding more human readable and comprehensible, and yet we still have demand for coders.  Instead of coding in R or Python (or binary), we'll soon be coding in English (or Spanish, Chinese, etc).  Coding will no longer be a barrier to statistical analysis, so junior and senior analysts can focus on the science!\n",
    "supporting": [
      "s01_ai_coding_support_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}