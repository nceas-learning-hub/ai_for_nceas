---
title: AI at the Command Line
---

:::{.callout-tip}
## Learning Objectives

- Understand what the MCP is and what Gemini CLI can do
- Apply prompts to read, analyze, and modify files using the Gemini agent
- Create and push content to GitHub with the help of the AI agent
:::

::: {.callout-exercise}
## üß™ warming up: Beyond Chat to Your Daily Tools {.unnumbered}

In this demo, we explore how Google Gemini can access personal productivity data such as:

- üìß Emails  
- üìÖ Calendars  
- ‚úÖ Task lists  

‚ö†Ô∏è **Important Note:** For security, Gemini (or any LLM agent) can only access these services if you explicitly grant permission and configure the integration. The Model Context Protocol (MCP) is used to control what the model can and cannot access.
:::


## What is MCP?

**MCP (Model Context Protocol)** is a standard for connecting AI models (like Gemini) with the outside world. It provides a safe and consistent way for AI agents to access:

- Local files on your computer  
- External tools (like GitHub, Google Workspace, Slack)  
- APIs or data services  

Think of MCP as a **bridge**: It defines how an AI model communicates with tools and resources so that interactions are reliable and reproducible.

![](images/ai_CLI/MCP.png){fig-align="center" width="100%"}
<small>Image credit: **Descope**</small>


::: {.callout-tip}

### Why MCP Matters

- Ensures **structured communication** between AI and external systems  
- Improves **safety** by controlling what the model can and cannot access  
- Makes it easier to **integrate AI into workflows** like coding, research, and publishing

:::


## From MCP to Gemini CLI

Now that you understand what **MCP (Model Context Protocol)** is, let‚Äôs see how it works in practice in addition to google workspace.

The [**Gemini CLI**](https://github.com/google-gemini/gemini-cli) is one of the first practical tools that implements MCP. It allows you to:

- Use Google‚Äôs Gemini AI models directly in your terminal  
- Access your local files
- Asking questions about code or data summary  
- Generating code snippets or documentation  
- Editing or refactoring code via prompt  
- GitHub integration: cloning, committing, pushing, issuing


::: {.callout-exercise}
## Publishing Code with Gemini CLI {.unnumbered}

- ‚öôÔ∏è **Installation & Setup**
  - Follow the detailed installation instruction on [Gemini CLI GitHub page](https://github.com/google-gemini/gemini-cli).

- üìÇ **Working with Local Files**
  - Generate a R script for downloading NOAA weather data
  - Edit the script with prompts

- üåê **GitHub Integration**
  - Create a GitHub repository
  - Generate or improve a README
  - Commit changes with Gemini‚Äôs assistance

:::

::: {.callout-tip}
## üõ† Troubleshooting {.unnumbered}

### üîë Authentication {.unnumbered} 
- When you first launch **Gemini CLI**, you‚Äôll be redirected to a webpage for authentication.  
- Free tier limits: **60 requests/min** and **1,000 requests/day**. If you exceed this, generate a [Gemini API key](https://aistudio.google.com/app/apikey) for more allowance.  
- Running Gemini CLI inside **VS Code**? You will be prompted to authorize VS Code to connect with Gemini CLI.

---

### üåê GitHub Integration {.unnumbered}
- If GitHub CLI (`gh`) is not installed, Gemini CLI will prompt you to install it. It is recommended to install it outside of Gemini CLI terminal.   
- The first run requires authenticating with your GitHub account. Example GitHub authentication screen below. The authentication token is obtained: GitHub settings ‚Üí Developer settings ‚Üí Personal access tokens

![](images/ai_CLI/githubcli.png){fig-align="center" width="90%"}{.unnumbered}
---

### ‚úÖ Verifying Changes {.unnumbered}
  - The tracked changes can be viewed in VS Code (Gemini CLI Companion extension) - or GitHub Desktop.
  - Or, in the terminal, run `git diff` to see what changes were made.

:::


## CLI Tools Comparison

AI command-line tools give researchers different ways to bring AI into their workflows.  
Here is a comparison of some of the most widely used ones:

<small>

| Tool            | Hosted vs Local | Best Use Cases |
|-----------------|-----------------|----------------|
| [**Gemini CLI**](https://github.com/google-gemini/gemini-cli)  | Cloud (Google)  |Best for individual experimentation because a substantial free tier, and it is lightweight |
| [**Codex CLI**](https://developers.openai.com/codex/cli) | Cloud (OpenAI) | Ideal for coding tasks (GPT-5), debugging, and automation|
| [**Claude Code**](https://claude.com/product/claude-code) | Cloud (Anthropic) | Best for handling large documents, summarization, and long-context reasoning |
| [**Ollama**](https://github.com/ollama/ollama)| Local (runs on your machine) | Perfect for privacy-first workflows, running offline models, and experimenting with open-source LLMs |

</small>

## Key Risks and Mitigation Strategies for Gemini CLI

Gemini CLI and GitHub CLI are powerful ‚Äî but with great power comes the responsibility to secure credentials, minimize permissions, and update regularly.

| üîë Risk | ‚úÖ Mitigation |
|---------|---------------|
| **Credential Exposure** | Use fine-grained tokens, avoid sharing on multi-user systems |
| **Accidental Actions** | double-check commands, avoid risky scripts |
| **Too Many Permissions** | Apply least privilege, audit tokens regularly |
| **Updates**|Keep CLI tools up to date |
| **Data Privacy** | Avoid sharing sensitive data, use local models for sensitive tasks |
